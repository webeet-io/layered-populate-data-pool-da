# -*- coding: utf-8 -*-
"""kindergartens_in_berlin_-data-transformation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kRxJhj3Yhdqq2xf_MLuk-3zqfCRdXA1y

**Step 2: Data Transformation**
"""

import os
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import io
import requests

# --- Configuration ---
# Define the directory where your raw data files are located
RAW_DATA_DIR = "/content"
PROCESSED_DATA_DIR = os.path.join(RAW_DATA_DIR, "processed")

# Ensure the processed data directory exists
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

# Define paths to your input data files
BERLIN_CSV_PATH = os.path.join(RAW_DATA_DIR, "berlin_kitas_raw.csv")

# A new, working GeoJSON URL for Berlin districts from a reliable source
NEIGHBOURHOODS_GEOJSON_URL = "https://raw.githubusercontent.com/m-hoerz/berlin-shapes/master/berliner-bezirke.geojson"

# --- 1. Load Data ---
print("--- Loading Data ---")

# Load Berlin Government Data (CSV)
berlin_df = pd.DataFrame()
try:
    # We now know the separator is a comma and the real header is on the second row (index 1)
    berlin_df = pd.read_csv(BERLIN_CSV_PATH, sep=',', encoding='utf-8', on_bad_lines='skip', header=1)

    # Clean up column names by stripping whitespace
    berlin_df.columns = berlin_df.columns.str.strip()

    print(f"Successfully loaded Berlin data from {BERLIN_CSV_PATH}")
    print("\nBerlin Data Columns:", berlin_df.columns.tolist())
except FileNotFoundError:
    print(f"Error: Berlin data file not found at {BERLIN_CSV_PATH}.")
    print("Please ensure 'berlin_kitas_raw.csv' is in the /content/ directory.")
except pd.errors.ParserError as e:
    print(f"ParserError loading Berlin data: {e}")
    print("Please check the structure of your CSV file.")
except Exception as e:
    print(f"An unexpected error occurred loading Berlin data: {e}")
    berlin_df = pd.DataFrame() # Ensure an empty DataFrame is created on error

# Load Berlin Neighbourhoods GeoJSON from URL
berlin_neighbourhoods = gpd.GeoDataFrame()
try:
    response = requests.get(NEIGHBOURHOODS_GEOJSON_URL)
    response.raise_for_status()  # Raise an error for bad status codes (e.g., 404)
    berlin_neighbourhoods = gpd.read_file(io.StringIO(response.text))
    print(f"\nSuccessfully loaded Berlin neighbourhoods GeoJSON from URL.")
except requests.exceptions.RequestException as e:
    print(f"\nAn error occurred loading neighbourhoods GeoJSON from URL: {e}")
except Exception as e:
    print(f"\nAn unexpected error occurred loading neighbourhoods GeoJSON: {e}")
    berlin_neighbourhoods = gpd.GeoDataFrame() # Ensure an empty GeoDataFrame on error

# --- 2. Data Cleaning and Transformation ---
print("\n--- Data Cleaning and Transformation ---")

if not berlin_df.empty:
    print("Processing Berlin data...")
    # Update the column mapping based on the corrected CSV header and umlauts
    column_map_berlin = {
        'Einrichtungsbezirk': 'district_code',
        'Einrichtungsbezirk Name': 'district',
        'Einrichtungsnummer': 'kita_id',
        'Einrichtungsname': 'name',
        'Stra√üe': 'street',
        'Hausnummer': 'street_number',
        'PLZ': 'postcode',
        'ETRS_YKOORDINATE': 'ETRS_y',
        'ETRS_XKOORDINATE': 'ETRS_x'
    }
    # Only rename columns that exist
    berlin_df.rename(columns={k: v for k, v in column_map_berlin.items() if k in berlin_df.columns}, inplace=True)

    if 'ETRS_x' in berlin_df.columns and 'ETRS_y' in berlin_df.columns:
        berlin_df['ETRS_x'] = pd.to_numeric(berlin_df['ETRS_x'], errors='coerce')
        berlin_df['ETRS_y'] = pd.to_numeric(berlin_df['ETRS_y'], errors='coerce')
        berlin_df.dropna(subset=['ETRS_x', 'ETRS_y'], inplace=True)
    else:
        print("Warning: ETRS coordinate columns not found. Skipping coordinate-based processing.")
        berlin_df = pd.DataFrame()  # Create empty dataframe to stop further processing

    if not berlin_df.empty:
        berlin_df['source'] = 'berlin_gov'
        selected_columns_berlin = ['name', 'street', 'postcode', 'ETRS_x', 'ETRS_y', 'source']
        berlin_processed = berlin_df[[col for col in selected_columns_berlin if col in berlin_df.columns]].copy()
        print("Berlin data processed.")
    else:
        berlin_processed = pd.DataFrame()
else:
    print("Skipping Berlin data processing due to loading errors.")
    berlin_processed = pd.DataFrame()

# --- 3. Create GeoDataFrame and Spatial Join ---
print("\n--- Creating GeoDataFrame and Performing Spatial Join ---")

final_gdf = gpd.GeoDataFrame()
if not berlin_processed.empty and not berlin_neighbourhoods.empty:
    # We now know the projection for the source data is ETRS89 / UTM zone 33N
    source_crs = "EPSG:25833"
    target_crs = "EPSG:4326"

    gdf = gpd.GeoDataFrame(
        berlin_processed,
        geometry=gpd.points_from_xy(berlin_processed['ETRS_x'], berlin_processed['ETRS_y']),
        crs=source_crs
    )

    # Re-project to the same CRS as the GeoJSON file
    gdf = gdf.to_crs(target_crs)

    try:
        # Perform the spatial join
        kitas_with_neighbourhoods = gpd.sjoin(gdf, berlin_neighbourhoods, how="inner", predicate="intersects")
        print("Spatial join complete.")

        neighbourhood_col_name = next((col for col in ['name', 'name_en', 'Name', 'neighbourhood', 'NAME_BEZIR', 'name_local'] if col in berlin_neighbourhoods.columns), None)

        if neighbourhood_col_name:
            final_gdf = kitas_with_neighbourhoods.rename(columns={neighbourhood_col_name: 'neighbourhood'})
            # Rename the converted coordinates to 'latitude' and 'longitude' for clarity
            final_gdf['longitude'] = final_gdf.geometry.x
            final_gdf['latitude'] = final_gdf.geometry.y
            final_gdf.drop(columns=['ETRS_x', 'ETRS_y'], inplace=True)
            print("Final GeoDataFrame with neighbourhood information created.")
        else:
            print("Could not find a suitable neighbourhood name column. Using the joined data as is.")
            final_gdf = kitas_with_neighbourhoods.copy()

    except Exception as e:
        print(f"An error occurred during the spatial join: {e}")

else:
    print("Skipping spatial join because either kindergarten data or neighbourhood data was not loaded/processed correctly.")

# --- 4. Save Processed Data ---
print("\n--- Saving Processed Data ---")

if not final_gdf.empty:
    output_geojson_path = os.path.join(PROCESSED_DATA_DIR, "berlin_kitas_processed.geojson")
    try:
        final_gdf.to_file(output_geojson_path, driver='GeoJSON')
        print(f"Successfully saved processed data to {output_geojson_path}")
    except Exception as e:
        print(f"An error occurred while saving GeoJSON: {e}")
        output_csv_path = os.path.join(PROCESSED_DATA_DIR, "berlin_kitas_processed.csv")
        final_gdf.drop(columns=['geometry'], errors='ignore').to_csv(output_csv_path, index=False)
        print(f"Saved to CSV instead at {output_csv_path}")
else:
    print("Final GeoDataFrame is empty. No data to save.")

print("\n--- Transformation Process Complete ---")