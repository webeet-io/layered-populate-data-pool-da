{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "\n",
        "\n",
        "class ImmoweltScraper:\n",
        "    \"\"\"\n",
        "    Immowelt production scraper using Selenium for JS rendering.\n",
        "    Extracts all field data directly from listing attributes,\n",
        "    based on latest Immowelt layout (2025).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, delay_range: tuple = (2, 5), headless: bool = False):\n",
        "        self.delay_range = delay_range\n",
        "        self.scraped_data = []\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Setup Selenium\n",
        "        options = Options()\n",
        "        if headless:\n",
        "            options.add_argument(\"--headless\")\n",
        "        self.driver = webdriver.Firefox(options=options)\n",
        "\n",
        "    def scrape_page(self, url: str) -> Optional[BeautifulSoup]:\n",
        "        \"\"\"Scrape a single page with Selenium and return BeautifulSoup object\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Opening {url}\")\n",
        "            self.driver.get(url)\n",
        "            time.sleep(random.uniform(*self.delay_range))  # human-like wait\n",
        "            html = self.driver.page_source\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            return soup\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to scrape {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_data(self, soup: BeautifulSoup) -> List[Dict]:\n",
        "        listings_data = []\n",
        "        links = soup.select('a[data-testid=\"card-mfe-covering-link-testid\"]')\n",
        "        images = soup.select('img[aria-label=\"Hauptbild\"]')\n",
        "\n",
        "        for i, link in enumerate(links):\n",
        "            try:\n",
        "                detail_url = link.get(\"href\")\n",
        "                title_attr = link.get(\"title\", \"\")\n",
        "                alt_text = images[i].get(\"alt\", \"\") if i < len(images) else \"\"\n",
        "                info_str = alt_text if alt_text else title_attr\n",
        "\n",
        "                listings_data.append({\n",
        "                    \"detail_url\": detail_url,\n",
        "                    \"raw_info\": info_str\n",
        "                })\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Error parsing a listing: {e}\")\n",
        "\n",
        "        self.logger.info(f\"Extracted {len(listings_data)} listings from page\")\n",
        "        return listings_data\n",
        "\n",
        "    def save_data(self, filename: str, fmt: str = \"csv\"):\n",
        "        \"\"\"Save scraped data to file\"\"\"\n",
        "        if not self.scraped_data:\n",
        "            self.logger.warning(\"No data to save\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.scraped_data)\n",
        "        if fmt.lower() == \"csv\":\n",
        "            df.to_csv(filename, index=False)\n",
        "        elif fmt.lower() == \"json\":\n",
        "            df.to_json(filename, orient=\"records\", indent=2)\n",
        "        elif fmt.lower() == \"excel\":\n",
        "            df.to_excel(filename, index=False)\n",
        "        self.logger.info(f\"Data saved to {filename}\")\n",
        "\n",
        "    def run_scraper(self, urls: List[str]):\n",
        "        \"\"\"Main loop to scrape multiple pages\"\"\"\n",
        "        self.logger.info(f\"Starting scraper for {len(urls)} URL(s)\")\n",
        "        for url in urls:\n",
        "            soup = self.scrape_page(url)\n",
        "            if soup:\n",
        "                page_data = self.extract_data(soup)\n",
        "                self.scraped_data.extend(page_data)\n",
        "        self.logger.info(f\"Scraping complete. Total listings: {len(self.scraped_data)}\")\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close Selenium browser\"\"\"\n",
        "        self.driver.quit()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL without page number\n",
        "    base_url_template = (\n",
        "        \"https://www.immowelt.de/classified-search\"\n",
        "        \"?distributionTypes=Rent\"\n",
        "        \"&estateTypes=House,Apartment\"\n",
        "        \"&locations=AD08DE8634\"\n",
        "        \"&projectTypes=New_Build,Flatsharing,Stock\"\n",
        "        \"&page={}\"\n",
        "    )\n",
        "\n",
        "    # Define how many pages you want to scrape\n",
        "    start_page = 1\n",
        "    end_page = 4  # change to full page count or desired limit, but better few pages for a run\n",
        "\n",
        "    # Create all page URLs\n",
        "    urls = [base_url_template.format(p) for p in range(start_page, end_page + 1)]\n",
        "\n",
        "    scraper = ImmoweltScraper(delay_range=(20, 40), headless=False) # Bigger delay helps avoid being blocked immediately\n",
        "\n",
        "    scraper.run_scraper(urls)\n",
        "    output_filename = f\"immowelt_pages_{start_page}_{end_page}.csv\"\n",
        "    scraper.save_data(output_filename, fmt=\"csv\")\n",
        "    scraper.close()\n",
        "    print(f\"âœ… Saved {output_filename}\")\n"
      ],
      "metadata": {
        "id": "hyQT3vL_ci0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}