{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a349862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 2.3.3\n",
      "pandas 2.3.2\n",
      "geopandas 1.1.1\n",
      "osmnx 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy, pandas, geopandas, osmnx\n",
    "print(\"numpy\", numpy.__version__)\n",
    "print(\"pandas\", pandas.__version__)\n",
    "print(\"geopandas\", geopandas.__version__)\n",
    "print(\"osmnx\", osmnx.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da6973d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=2\n",
      "  Using cached numpy-2.3.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.2.3)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: geopandas>=0.14 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.14.4)\n",
      "Collecting geopandas>=0.14\n",
      "  Using cached geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: shapely>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.0.7)\n",
      "Collecting shapely>=2\n",
      "  Using cached shapely-2.1.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pyproj>=3.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (3.7.2)\n",
      "Collecting rtree\n",
      "  Using cached rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting osmnx>=2.0\n",
      "  Using cached osmnx-2.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from geopandas>=0.14) (0.11.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from geopandas>=0.14) (25.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pyproj>=3.5) (2025.8.3)\n",
      "Requirement already satisfied: networkx>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from osmnx>=2.0) (3.3)\n",
      "Requirement already satisfied: requests>=2.27 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from osmnx>=2.0) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.27->osmnx>=2.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.27->osmnx>=2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.27->osmnx>=2.0) (2.5.0)\n",
      "Using cached numpy-2.3.3-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pandas-2.3.2-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
      "Using cached shapely-2.1.1-cp311-cp311-macosx_11_0_arm64.whl (1.6 MB)\n",
      "Using cached rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
      "Using cached osmnx-2.0.6-py3-none-any.whl (101 kB)\n",
      "Installing collected packages: rtree, numpy, shapely, pandas, geopandas, osmnx\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[2K  Attempting uninstall: shapely0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: shapely 2.0.7━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling shapely-2.0.7:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled shapely-2.0.7━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: pandas╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [shapely]\n",
      "\u001b[2K    Found existing installation: pandas 2.2.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [shapely]\n",
      "\u001b[2K    Uninstalling pandas-2.2.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.2.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: geopandas[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K    Found existing installation: geopandas 0.14.4━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K    Uninstalling geopandas-0.14.4:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled geopandas-0.14.4━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: osmnx━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [geopandas]\n",
      "\u001b[2K    Found existing installation: osmnx 1.9.4\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [geopandas]\n",
      "\u001b[2K    Uninstalling osmnx-1.9.4:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [geopandas]\n",
      "\u001b[2K      Successfully uninstalled osmnx-1.9.40m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [geopandas]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [osmnx]/6\u001b[0m [geopandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed geopandas-1.1.1 numpy-2.3.3 osmnx-2.0.6 pandas-2.3.2 rtree-1.4.1 shapely-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade \"numpy>=2\" pandas \"geopandas>=0.14\" \"shapely>=2\" \"pyproj>=3.5\" rtree \"osmnx>=2.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb885a6b",
   "metadata": {},
   "source": [
    "OSM DATENEXTRAKTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469f8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved berlin_vet_clinics_osm.csv with 170 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/z3f134vj17zg0vm9xq6gmy4h0000gn/T/ipykernel_57787/1838105717.py:67: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  out[\"lat\"] = gdf.geometry.centroid.y\n",
      "/var/folders/f2/z3f134vj17zg0vm9xq6gmy4h0000gn/T/ipykernel_57787/1838105717.py:68: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  out[\"lon\"] = gdf.geometry.centroid.x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install -U \"osmnx>=2.0\" geopandas shapely pandas\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "\n",
    "# ---- choose the right features function for your OSMnx version ----\n",
    "try:\n",
    "    # OSMnx 2.x\n",
    "    from osmnx.features import features_from_place as get_features\n",
    "except Exception:\n",
    "    # OSMnx 1.x fallback\n",
    "    get_features = ox.geometries_from_place\n",
    "\n",
    "place = \"Berlin, Germany\"\n",
    "tags = {\"amenity\": \"veterinary\"}\n",
    "\n",
    "# ---- query OSM ----\n",
    "gdf = get_features(place, tags).copy().reset_index()  # puts 'osmid' into columns\n",
    "\n",
    "\n",
    "# Ensure we have an 'osmid' column regardless of OSMnx version/index layout\n",
    "if \"osmid\" not in gdf.columns:\n",
    "    idx = gdf.index\n",
    "    if isinstance(idx, pd.MultiIndex) and \"osmid\" in idx.names:\n",
    "        # OSMnx ≥2: MultiIndex (element_type, osmid) → make them columns\n",
    "        gdf = gdf.reset_index()  # adds 'element_type' and 'osmid'\n",
    "    elif idx.name == \"osmid\":\n",
    "        # Some 1.x builds: single index named 'osmid'\n",
    "        gdf = gdf.reset_index()\n",
    "    elif any(c in gdf.columns for c in [\"id\", \"@id\", \"osm_id\", \"way_id\", \"relation_id\", \"node_id\"]):\n",
    "        # Rare layouts: rename whichever ID column exists\n",
    "        for c in [\"id\", \"@id\", \"osm_id\", \"way_id\", \"relation_id\", \"node_id\"]:\n",
    "            if c in gdf.columns:\n",
    "                gdf = gdf.rename(columns={c: \"osmid\"})\n",
    "                break\n",
    "    else:\n",
    "        # Fallback: create empty osmid (should be rare)\n",
    "        gdf.insert(0, \"osmid\", pd.NA)\n",
    "\n",
    "\n",
    "# ---- helper: coalesce multiple possible tag columns into one ----\n",
    "def coalesce(*series):\n",
    "    out = None\n",
    "    for s in series:\n",
    "        if s is not None:\n",
    "            out = s if out is None else out.fillna(s)\n",
    "    return out\n",
    "\n",
    "# ---- build output with stable columns ----\n",
    "base_cols = [\n",
    "    \"osmid\", \"name\", \"amenity\",\n",
    "    \"addr:street\", \"addr:housenumber\", \"addr:postcode\", \"addr:city\",\n",
    "    \"opening_hours\", \"operator\", \"brand\",\n",
    "    \"veterinary:speciality\", \"wheelchair\", \"emergency\",\n",
    "]\n",
    "existing = [c for c in base_cols if c in gdf.columns]\n",
    "out = gdf[existing].copy()\n",
    "\n",
    "# normalized contact fields (pull from either contact:* or plain)\n",
    "out[\"phone\"]   = coalesce(gdf.get(\"contact:phone\"),   gdf.get(\"phone\"))\n",
    "out[\"website\"] = coalesce(gdf.get(\"contact:website\"), gdf.get(\"website\"))\n",
    "out[\"email\"]   = coalesce(gdf.get(\"contact:email\"),   gdf.get(\"email\"))\n",
    "out[\"operator\"] = coalesce(gdf.get(\"operator\"), gdf.get(\"brand\"))\n",
    "\n",
    "# lat/lon from centroid (works for points/polygons)\n",
    "out[\"lat\"] = gdf.geometry.centroid.y\n",
    "out[\"lon\"] = gdf.geometry.centroid.x\n",
    "\n",
    "# order final columns (only those that exist)\n",
    "final_cols = [\n",
    "    \"osmid\",\"name\",\"amenity\",\n",
    "    \"addr:street\",\"addr:housenumber\",\"addr:postcode\",\"addr:city\",\n",
    "    \"phone\",\"website\",\"email\",\n",
    "    \"opening_hours\",\"operator\",\"brand\",\n",
    "    \"veterinary:speciality\",\"wheelchair\",\"emergency\",\n",
    "    \"lat\",\"lon\",\n",
    "]\n",
    "out = out.reindex(columns=[c for c in final_cols if c in out.columns])\n",
    "\n",
    "# ---- write CSV ----\n",
    "out.to_csv(\"berlin_vet_clinics_osm.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Saved berlin_vet_clinics_osm.csv with\", len(out), \"rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d884eb",
   "metadata": {},
   "source": [
    "TIERÄRZTEKAMMER BERLIN DATENEXTRAKTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a8df2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "   name                                          full_text address  \\\n",
      "0  None  Klinik für Klein- und Heimtiere, Alt-Biesdorf ...    None   \n",
      "1  None  Klinik für Kleintiere (Olof Löwe), Märkische A...    None   \n",
      "2  None  valera – Medizinisches Kleintierzentrum Berlin...    None   \n",
      "3  None  Tierarztpraxis Bärenwiese, Uhlandstr 151, 1071...    None   \n",
      "4  None  Tierarztpraxis Rödiger, Scharnweberstr. 136, 1...    None   \n",
      "\n",
      "              phone opening_hours_raw  \n",
      "0     030 51 43 760              None  \n",
      "1     030 93 22 093              None  \n",
      "2  030 201 80 57 50              None  \n",
      "3   030 23 36 26 27              None  \n",
      "4     030 412 73 57              None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/z3f134vj17zg0vm9xq6gmy4h0000gn/T/ipykernel_51537/54872285.py:45: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  oh_el = card.find(text=re.compile(\"Öffnungs|opening\", re.I))\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/\n",
    "%pip install beautifulsoup4 lxml\n",
    "\n",
    "import time, re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"research-bot/1.0 (contact: you@example.com)\"}\n",
    "URL = \"https://tieraerztekammer-berlin.de/notdienst/\"  \n",
    "\n",
    "def clean_text(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def scrape_taek_berlin_emergency(url=URL):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Find the main container that lists clinics (adjust selector to the page)\n",
    "    container = soup.select_one(\"main, .content, .article\") or soup\n",
    "    cards = container.select(\".clinic, .entry, article, li\")  # be flexible\n",
    "\n",
    "    rows = []\n",
    "    for card in cards:\n",
    "        txt = clean_text(card.get_text(\" \", strip=True))\n",
    "\n",
    "        name_el = card.select_one(\"h2, h3, .title, .clinic-name\")\n",
    "        name = clean_text(name_el.get_text()) if name_el else None\n",
    "\n",
    "        addr = None\n",
    "        addr_el = card.select_one(\".address, address, .adr\")\n",
    "        if addr_el:\n",
    "            addr = clean_text(addr_el.get_text(\" \"))\n",
    "\n",
    "        phone = None\n",
    "        tel_el = card.select_one(\"a[href^='tel'], .phone, .tel\")\n",
    "        if tel_el:\n",
    "            phone = clean_text(tel_el.get_text() or tel_el.get(\"href\"))\n",
    "        else:\n",
    "            m = re.search(r\"(?:\\+49|0)\\s?[\\d ()\\-\\/]{6,}\", txt)\n",
    "            phone = m.group(0) if m else None\n",
    "\n",
    "        oh = None\n",
    "        oh_el = card.find(text=re.compile(\"Öffnungs|opening\", re.I))\n",
    "        if oh_el:\n",
    "            oh = clean_text(oh_el.parent.get_text(\" \"))\n",
    "\n",
    "        rows.append({\n",
    "            \"name\": name or None,\n",
    "            \"full_text\": txt,\n",
    "            \"address\": addr,\n",
    "            \"phone\": phone,\n",
    "            \"opening_hours_raw\": oh\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = scrape_taek_berlin_emergency()\n",
    "df.to_csv(\"taek_berlin_emergency.csv\", index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5259e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           full_text           street  \\\n",
      "0  Klinik für Klein- und Heimtiere, Alt-Biesdorf ...     Alt-Biesdorf   \n",
      "1  Klinik für Kleintiere (Olof Löwe), Märkische A...  Märkische Allee   \n",
      "2  valera – Medizinisches Kleintierzentrum Berlin...   Potsdamer Str.   \n",
      "\n",
      "  house_number postcode    city    phone_number email website opening_hours  \\\n",
      "0           22    12683  Berlin    +49305143760  None    None          None   \n",
      "1          258    12679  Berlin    +49309322093  None    None          None   \n",
      "2           23    14163  Berlin  +4930201805750  None    None          None   \n",
      "\n",
      "                        full_address  \n",
      "0      Alt-Biesdorf 22, 12683 Berlin  \n",
      "1  Märkische Allee 258, 12679 Berlin  \n",
      "2    Potsdamer Str. 23, 14163 Berlin  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1) Load your uploaded CSV\n",
    "df = pd.read_csv(\"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency.csv\", encoding=\"utf-8\")\n",
    "assert \"full_text\" in df.columns, \"full_text column missing\"\n",
    "\n",
    "# --- Regexes (tune for your data) ---\n",
    "RE_EMAIL   = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "RE_WEBSITE = r'(https?://[^\\s,;]+|www\\.[^\\s,;]+)'\n",
    "RE_PHONE   = r'(?:(?:\\+?\\s?49)|(?:\\+?\\s?49\\(0\\))|(?:0))[\\s()/\\-]*\\d[\\d\\s()/\\-]{5,}'\n",
    "RE_POSTCODE_CITY = r'(?P<postcode>\\b\\d{5}\\b)\\s+(?P<city>[A-Za-zÄÖÜäöüß\\-\\s]+)'\n",
    "RE_STREET  = r'(?P<street>[A-Za-zÄÖÜäöüß\\.\\-\\s]+?)\\s+(?P<housenumber>\\d+[A-Za-z]?)'\n",
    "RE_OPENING = r'(Öffnungszeiten|Opening hours|Öffnung|hours)\\s*[:\\-]?\\s*(?P<opening_hours>[^•\\|;]+)'\n",
    "\n",
    "def first_match(pattern, text, flags=re.IGNORECASE):\n",
    "    if pd.isna(text): return None\n",
    "    m = re.search(pattern, str(text), flags)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def extract_group(pattern, text, group, flags=re.IGNORECASE):\n",
    "    if pd.isna(text): return None\n",
    "    m = re.search(pattern, str(text), flags)\n",
    "    return m.group(group) if m else None\n",
    "\n",
    "def norm_space(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip() if isinstance(s, str) else s\n",
    "\n",
    "def normalize_phone(p):\n",
    "    if not p: return None\n",
    "    s = re.sub(r\"[^\\d+]\", \"\", p)\n",
    "    if s.startswith(\"0\"):  # naive DE normalization\n",
    "        s = \"+49\" + s[1:]\n",
    "    return s\n",
    "\n",
    "def normalize_url(u):\n",
    "    if not u: return None\n",
    "    return u if u.startswith((\"http://\",\"https://\")) else \"https://\" + u\n",
    "\n",
    "# --- Extract ---\n",
    "s = df[\"full_text\"].fillna(\"\")\n",
    "\n",
    "df[\"email\"]          = s.map(lambda t: first_match(RE_EMAIL, t))\n",
    "df[\"website\"]        = s.map(lambda t: normalize_url(first_match(RE_WEBSITE, t)))\n",
    "df[\"phone_number\"]   = s.map(lambda t: normalize_phone(first_match(RE_PHONE, t)))\n",
    "df[\"opening_hours\"]  = s.map(lambda t: extract_group(RE_OPENING, t, \"opening_hours\"))\n",
    "\n",
    "df[\"street\"]         = s.map(lambda t: extract_group(RE_STREET, t, \"street\")).map(norm_space)\n",
    "df[\"house_number\"]   = s.map(lambda t: extract_group(RE_STREET, t, \"housenumber\")).map(norm_space)\n",
    "df[\"postcode\"]       = s.map(lambda t: extract_group(RE_POSTCODE_CITY, t, \"postcode\"))\n",
    "df[\"city\"]           = s.map(lambda t: extract_group(RE_POSTCODE_CITY, t, \"city\")).map(norm_space)\n",
    "\n",
    "# Build full_address from parts (only non-null pieces)\n",
    "def compose_address(row):\n",
    "    a = \" \".join([x for x in [row.street, row.house_number] if pd.notna(x) and x])\n",
    "    b = \" \".join([x for x in [row.postcode, row.city] if pd.notna(x) and x])\n",
    "    return \", \".join([x for x in [a, b] if x]) or None\n",
    "\n",
    "df[\"full_address\"] = df.apply(compose_address, axis=1)\n",
    "\n",
    "# Optional: clean up whitespace in extracted columns\n",
    "for col in [\"email\",\"website\",\"phone_number\",\"opening_hours\",\"street\",\"house_number\",\"postcode\",\"city\",\"full_address\"]:\n",
    "    df[col] = df[col].map(norm_space)\n",
    "\n",
    "# --- Save result ---\n",
    "df.to_csv(\"taek_berlin_emergency_parsed.csv\", index=False)\n",
    "print(df.head(3)[[\"full_text\",\"street\",\"house_number\",\"postcode\",\"city\",\"phone_number\",\"email\",\"website\",\"opening_hours\",\"full_address\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de625f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# German day abbreviations\n",
    "DAYS_DE = r\"(?:Mo|Di|Mi|Do|Fr|Sa|So)\"\n",
    "\n",
    "# OSM-ish compact patterns like: \"Mo-Fr 09:00-18:00; Sa 10:00-14:00\"\n",
    "RE_HOURS_OSMISH = re.compile(\n",
    "    rf\"\\b{DAYS_DE}(?:[,\\-/ ]\\s*{DAYS_DE})*\\s+\\d{{1,2}}[:.]?\\d{{2}}\\s*-\\s*\\d{{1,2}}[:.]?\\d{{2}}\"\n",
    "    rf\"(?:\\s*;\\s*{DAYS_DE}(?:[,\\-/ ]\\s*{DAYS_DE})*\\s+\\d{{1,2}}[:.]?\\d{{2}}\\s*-\\s*\\d{{1,2}}[:.]?\\d{{2}})*\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Labeled variants like \"Öffnungszeiten: Mo–Fr 9-18 Uhr\" or \"Sprechzeiten - ...\"\n",
    "RE_HOURS_LABELED = re.compile(\n",
    "    r\"(Öffnungs(?:zeiten|zeit)|Sprechzeiten?|Sprechstunde|Opening hours)\\s*[:\\-–]?\\s*(?P<label_hours>[^\\n\\r|•;]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Common \"emergency\" cues\n",
    "RE_EMERGENCY_FLAG = re.compile(\n",
    "    r\"\\b(Notfall(?:e)?|Notfälle|Notdienst|Notfallsprechstunde|Notaufnahme|24\\s*h|24h|24\\s*Std\\.?|rund um die Uhr|emergency)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Labeled emergency details like \"Notdienst: 24h\" or \"Notfälle – Tel. 030 ...\"\n",
    "RE_EMERGENCY_DETAILS = re.compile(\n",
    "    r\"(Notdienst|Notfälle?|Notfallsprechstunde)\\s*[:\\-–]?\\s*(?P<em_details>[^\\n\\r|•;]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Optional phone pattern if you want to capture emergency phone on the same line\n",
    "RE_PHONE = re.compile(r\"(?:\\+49|0)[\\d\\s()/\\-]{6,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7dc8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_opening_hours(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    # Prefer structured/OSM-like first\n",
    "    m = RE_HOURS_OSMISH.search(text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    # Fallback to labeled phrase\n",
    "    m2 = RE_HOURS_LABELED.search(text)\n",
    "    if m2:\n",
    "        return m2.group(\"label_hours\").strip()\n",
    "    return None\n",
    "\n",
    "def extract_emergency(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return False, None, None\n",
    "    flag = bool(RE_EMERGENCY_FLAG.search(text))\n",
    "    details = None\n",
    "    phone = None\n",
    "\n",
    "    m = RE_EMERGENCY_DETAILS.search(text)\n",
    "    if m:\n",
    "        details = m.group(\"em_details\").strip()\n",
    "        # try to find a phone number inside the details\n",
    "        pm = RE_PHONE.search(details)\n",
    "        if pm:\n",
    "            phone = pm.group(0).strip()\n",
    "\n",
    "    return flag, details, phone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6c0d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"your_file.csv\", encoding=\"utf-8\")\n",
    "assert \"full_text\" in df.columns, \"full_text column missing\"\n",
    "\n",
    "# Opening hours\n",
    "df[\"opening_hours_raw\"] = df[\"full_text\"].apply(extract_opening_hours)\n",
    "\n",
    "# Emergency fields\n",
    "out = df[\"full_text\"].apply(extract_emergency)\n",
    "df[\"emergency_flag\"]    = out.apply(lambda t: t[0])\n",
    "df[\"emergency_details\"] = out.apply(lambda t: (t[1] or None))\n",
    "df[\"emergency_phone\"]   = out.apply(lambda t: (t[2] or None))\n",
    "df.to_csv(\"taek_berlin_emergency_parsed_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d27d63fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved without overwriting v2 → taek_berlin_emergency_parsed_v4.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0) Load existing file (v2) ---\n",
    "\n",
    "df = pd.read_csv(\"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency.csv\", encoding=\"utf-8\")\n",
    "assert \"full_text\" in df.columns, \"full_text column missing in v2 file\"\n",
    "\n",
    "# --- 1) Regex patterns (tuned for your German examples) ---\n",
    "DAYS_DE  = r\"(?:Mo|Di|Mi|Do|Fr|Sa|So)\"\n",
    "TIME     = r\"\\d{1,2}[:.]?\\d{0,2}\\s*(?:Uhr)?\"\n",
    "RANGE    = rf\"{TIME}\\s*[-–]\\s*{TIME}\"\n",
    "RANGE_OR = rf\"{RANGE}(?:\\s*(?:u\\.?|und)\\s*{RANGE})*\"\n",
    "\n",
    "RE_HOURS_BLOCK = re.compile(\n",
    "    rf\"(?:(?:{DAYS_DE})(?:\\s*[-–]\\s*{DAYS_DE})?\\s+{RANGE_OR})\"\n",
    "    rf\"(?:\\s*(?:[|;,\\n]\\s*|\\s{DAYS_DE}\\s))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_HOURS_LABELED = re.compile(\n",
    "    r\"(Öffnungs(?:zeiten|zeit)|Sprechzeiten?|Sprechstunde|Opening hours)\\s*[:\\-–]?\\s*(?P<label_hours>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_FLAG = re.compile(\n",
    "    r\"\\b(?:Notdienst|Notfälle?|Notfallsprechstunde|Notaufnahme|Feiertagsnotdienst|24\\s*h|24h|rund um die Uhr|emergency)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_DETAILS = re.compile(\n",
    "    r\"(Notdienst|Notfälle?|Notfallsprechstunde|Feiertagsnotdienst)\\s*[:\\-–]?\\s*(?P<em_details>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_opening_hours(text: str):\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    blocks = [m.group(0).strip(\" ,;|\") for m in RE_HOURS_BLOCK.finditer(text)]\n",
    "    if blocks:\n",
    "        return \" | \".join(b for b in blocks if b)\n",
    "    m = RE_HOURS_LABELED.search(text)\n",
    "    return m.group(\"label_hours\").strip() if m else None\n",
    "\n",
    "def extract_emergency(text: str):\n",
    "    if not isinstance(text, str): \n",
    "        return False, None\n",
    "    flag = bool(RE_EMERGENCY_FLAG.search(text))\n",
    "    details = None\n",
    "    m = RE_EMERGENCY_DETAILS.search(text)\n",
    "    if m:\n",
    "        details = m.group(\"em_details\").strip(\" ,;|\")\n",
    "    return flag, details\n",
    "\n",
    "# --- 2) Compute new values (from existing full_text) ---\n",
    "s = df[\"full_text\"].fillna(\"\")\n",
    "new_hours = s.map(extract_opening_hours)\n",
    "new_em    = s.map(extract_emergency)\n",
    "new_flag  = new_em.map(lambda x: x[0])\n",
    "new_det   = new_em.map(lambda x: x[1])\n",
    "\n",
    "# --- 3) Add/adjust without overwriting existing non-null values ---\n",
    "for col, series in {\n",
    "    \"opening_hours_raw\": new_hours,\n",
    "    \"emergency_flag\":    new_flag,\n",
    "    \"emergency_details\": new_det,\n",
    "}.items():\n",
    "    if col not in df.columns:\n",
    "        df[col] = series                      # add fresh\n",
    "    else:\n",
    "        # only fill where currently missing/NaN; keep existing values\n",
    "        df[col] = df[col].where(df[col].notna(), series)\n",
    "\n",
    "# --- 4) Save to a NEW versioned file (v3, v4, …) ---\n",
    "base = in_path.stem.replace(\"_v2\", \"\")  # handle typical naming\n",
    "parent = in_path.parent\n",
    "version = 3\n",
    "while True:\n",
    "    out_path = parent / f\"{base}_v{version}.csv\"\n",
    "    if not out_path.exists():\n",
    "        break\n",
    "    version += 1\n",
    "\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Saved without overwriting v2 → {out_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45fad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2 cols: 17\n",
      "v3 cols: 7\n",
      "Missing in v3: ['city', 'email', 'emergency_phone', 'full_address', 'house_number', 'opening_hours', 'phone_number', 'postcode', 'street', 'website']\n",
      "New in v3: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "v2 = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v2.csv\"\n",
    "v3 = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v3.csv\"\n",
    "\n",
    "df2 = pd.read_csv(v2, encoding=\"utf-8\")\n",
    "df3 = pd.read_csv(v3, encoding=\"utf-8\")\n",
    "\n",
    "print(\"v2 cols:\", len(df2.columns))\n",
    "print(\"v3 cols:\", len(df3.columns))\n",
    "print(\"Missing in v3:\", sorted(set(df2.columns) - set(df3.columns)))\n",
    "print(\"New in v3:\", sorted(set(df3.columns) - set(df2.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb153f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v3.csv\n"
     ]
    }
   ],
   "source": [
    "import re, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "v2_path = Path( \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v2.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(v2_path, encoding=\"utf-8\")\n",
    "\n",
    "assert \"full_text\" in df.columns, \"v2 must contain full_text\"\n",
    "\n",
    "# ---- regex (your earlier extractors) ----\n",
    "DAYS_DE  = r\"(?:Mo|Di|Mi|Do|Fr|Sa|So)\"\n",
    "TIME     = r\"\\d{1,2}[:.]?\\d{0,2}\\s*(?:Uhr)?\"\n",
    "RANGE    = rf\"{TIME}\\s*[-–]\\s*{TIME}\"\n",
    "RANGE_OR = rf\"{RANGE}(?:\\s*(?:u\\.?|und)\\s*{RANGE})*\"\n",
    "\n",
    "RE_HOURS_BLOCK = re.compile(\n",
    "    rf\"(?:(?:{DAYS_DE})(?:\\s*[-–]\\s*{DAYS_DE})?\\s+{RANGE_OR})(?:\\s*(?:[|;,\\n]\\s*|\\s{DAYS_DE}\\s))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_HOURS_LABELED = re.compile(\n",
    "    r\"(Öffnungs(?:zeiten|zeit)|Sprechzeiten?|Sprechstunde|Opening hours)\\s*[:\\-–]?\\s*(?P<label_hours>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_FLAG = re.compile(\n",
    "    r\"\\b(?:Notdienst|Notfälle?|Notfallsprechstunde|Notaufnahme|Feiertagsnotdienst|24\\s*h|24h|rund um die Uhr|emergency)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_DETAILS = re.compile(\n",
    "    r\"(Notdienst|Notfälle?|Notfallsprechstunde|Feiertagsnotdienst)\\s*[:\\-–]?\\s*(?P<em_details>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_opening_hours(text: str):\n",
    "    if not isinstance(text, str): return None\n",
    "    blocks = [m.group(0).strip(\" ,;|\") for m in RE_HOURS_BLOCK.finditer(text)]\n",
    "    if blocks:\n",
    "        return \" | \".join(b for b in blocks if b)\n",
    "    m = RE_HOURS_LABELED.search(text)\n",
    "    return m.group(\"label_hours\").strip() if m else None\n",
    "\n",
    "def extract_emergency(text: str):\n",
    "    if not isinstance(text, str): return False, None\n",
    "    flag = bool(RE_EMERGENCY_FLAG.search(text))\n",
    "    details = None\n",
    "    m = RE_EMERGENCY_DETAILS.search(text)\n",
    "    if m:\n",
    "        details = m.group(\"em_details\").strip(\" ,;|\")\n",
    "    return flag, details\n",
    "\n",
    "# ---- compute new series from full_text ----\n",
    "s = df[\"full_text\"].fillna(\"\")\n",
    "new_hours = s.map(extract_opening_hours)\n",
    "new_flag, new_det = zip(*s.map(extract_emergency))\n",
    "\n",
    "# ---- add/only-fill (never drop, never overwrite non-nulls) ----\n",
    "def add_or_fill(col, series):\n",
    "    if col not in df.columns:\n",
    "        df[col] = series\n",
    "    else:\n",
    "        df[col] = df[col].where(df[col].notna(), pd.Series(series))\n",
    "\n",
    "add_or_fill(\"opening_hours_raw\", new_hours)\n",
    "add_or_fill(\"emergency_flag\",   list(new_flag))\n",
    "add_or_fill(\"emergency_details\",list(new_det))\n",
    "\n",
    "# ---- save to a NEW file, all columns preserved ----\n",
    "out_path = v2_path.with_name(\"taek_berlin_emergency_parsed_v3.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edaa9e1",
   "metadata": {},
   "source": [
    "BPT DATENEXTRAKTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fcae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c524b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 lxml pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59290248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Seiten: 9\n",
      "Seite 1: +10\n",
      "Seite 2: +10\n",
      "Seite 3: +10\n",
      "Seite 4: +10\n",
      "Seite 5: +10\n",
      "Seite 6: +10\n",
      "Seite 7: +10\n",
      "Seite 8: +10\n",
      "Seite 9: +4\n",
      "✅ Gespeichert: bpt_tierarztsuche_berlin.csv – 84 Zeilen\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "START_URL = \"https://www.tieraerzteverband.de/bpt/ueber-den-bpt/tierarztsuche/index.php?name=&zipcode=&town=berlin&radius=100\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Safari/537.36\",\n",
    "    \"Accept-Language\": \"de-DE,de;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "PHONE_CLEAN_RE = re.compile(r\"[^\\d+()\\s/\\-]\")\n",
    "POSTCODE_CITY_RE = re.compile(r\"\\b(\\d{5})\\s+(.+)\")\n",
    "STREET_RE = re.compile(r\".*\\d+\\w?$\")  # Zeile mit Hausnummer\n",
    "\n",
    "def with_page(url, p):\n",
    "    \"\"\"Add/replace ?p=... in URL.\"\"\"\n",
    "    parts = urlparse(url)\n",
    "    q = parse_qs(parts.query)\n",
    "    q[\"p\"] = [str(p)]\n",
    "    new_q = urlencode({k: v[0] for k, v in q.items()})\n",
    "    return urlunparse(parts._replace(query=new_q))\n",
    "\n",
    "def total_pages(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    sel = soup.select_one('form[name^=\"pageNaviList\"] select[name=\"p\"]')\n",
    "    if not sel:\n",
    "        return 1\n",
    "    opts = sel.select(\"option\")\n",
    "    return max(int(o.get(\"value\", \"1\")) for o in opts) if opts else 1\n",
    "\n",
    "def text_lines(el):\n",
    "    return [ln.strip() for ln in el.get_text(\"\\n\", strip=True).split(\"\\n\") if ln.strip()]\n",
    "\n",
    "def parse_latlon_from_next_script(result_div):\n",
    "    # Suche das nächste <script> nach diesem Ergebnis, das 'var longtitude' enthält\n",
    "    sc = result_div.find_next(\"script\")\n",
    "    tries = 0\n",
    "    while sc and tries < 5:\n",
    "        t = sc.string or sc.get_text()\n",
    "        if t and \"var longtitude\" in t and \"var latitude\" in t:\n",
    "            m = re.search(r\"longtitude\\s*=\\s*'([\\d\\.]+)';\\s*var\\s+latitude\\s*=\\s*'([\\d\\.]+)'\", t)\n",
    "            if m:\n",
    "                return float(m.group(2)), float(m.group(1))  # lat, lon\n",
    "        sc = sc.find_next(\"script\")\n",
    "        tries += 1\n",
    "    return None, None\n",
    "\n",
    "def parse_results(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    rows = []\n",
    "    for card in soup.select(\"div.elementStandard.elementResultLine.mgStyle\"):\n",
    "        # Name\n",
    "        name = (card.select_one(\".headline h2\") or card).get_text(\" \", strip=True)\n",
    "\n",
    "        # Cols\n",
    "        col1 = card.select_one(\".columns .col1\")\n",
    "        col2 = card.select_one(\".columns .col2\")\n",
    "        col3 = card.select_one(\".columns .col3\")\n",
    "\n",
    "        practice = street = postcode = city = distance = None\n",
    "\n",
    "        if col1:\n",
    "            lines = text_lines(col1)\n",
    "            # Entfernung\n",
    "            for ln in lines:\n",
    "                if ln.lower().startswith(\"entfernung\"):\n",
    "                    distance = ln.replace(\"Entfernung:\", \"\").strip()\n",
    "            # Postcode & City\n",
    "            for ln in lines:\n",
    "                m = POSTCODE_CITY_RE.search(ln)\n",
    "                if m:\n",
    "                    postcode, city = m.group(1), m.group(2).strip()\n",
    "            # Street line: letzte Zeile vor PLZ, die Hausnummer enthält\n",
    "            if postcode:\n",
    "                # take the line directly before the postcode line that has a number\n",
    "                for i, ln in enumerate(lines):\n",
    "                    if POSTCODE_CITY_RE.search(ln) and i > 0:\n",
    "                        candidate = lines[i-1]\n",
    "                        if STREET_RE.match(candidate):\n",
    "                            street = candidate\n",
    "                        break\n",
    "            # Praxis/Einrichtung: meist erste Zeile (ohne Entfernung/PLZ/Street)\n",
    "            if lines:\n",
    "                first = lines[0]\n",
    "                if not POSTCODE_CITY_RE.search(first) and not first.lower().startswith(\"entfernung\"):\n",
    "                    practice = first\n",
    "\n",
    "        phone_list, email, website = [], None, None\n",
    "        if col2:\n",
    "            for a in col2.select(\"a.phone, a.mobile\"):\n",
    "                ph = a.get_text(\" \", strip=True)\n",
    "                ph = PHONE_CLEAN_RE.sub(\"\", ph).strip()\n",
    "                if ph and ph not in phone_list:\n",
    "                    phone_list.append(ph)\n",
    "            a_mail = col2.select_one('a.wpst[href^=\"mailto:\"]')\n",
    "            if a_mail:\n",
    "                email = a_mail.get(\"href\").split(\"mailto:\")[-1]\n",
    "            a_www = col2.select_one('a.www[href^=\"http\"]')\n",
    "            if a_www:\n",
    "                website = a_www.get(\"href\")\n",
    "\n",
    "        species = None\n",
    "        if col3:\n",
    "            checks = [c.get_text(\" \", strip=True) for c in col3.select(\".checkbox\")]\n",
    "            if checks:\n",
    "                species = \"; \".join(checks)\n",
    "\n",
    "        lat, lon = parse_latlon_from_next_script(card)\n",
    "\n",
    "        rows.append({\n",
    "            \"name\": name or None,\n",
    "            \"practice\": practice,\n",
    "            \"street\": street,\n",
    "            \"postcode\": postcode,\n",
    "            \"city\": city,\n",
    "            \"distance\": distance,\n",
    "            \"phone\": \" / \".join(phone_list) if phone_list else None,\n",
    "            \"email\": email,\n",
    "            \"website\": website,\n",
    "            \"species\": species,\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def scrape_all(start_url=START_URL, out_csv=\"bpt_tierarztsuche_berlin.csv\"):\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "\n",
    "    # 1) Erste Seite laden, Seitenzahl bestimmen\n",
    "    r0 = s.get(start_url, timeout=30)\n",
    "    r0.raise_for_status()\n",
    "    n_pages = total_pages(r0.text)\n",
    "    print(f\"Gefundene Seiten: {n_pages}\")\n",
    "    all_rows = parse_results(r0.text)\n",
    "    print(f\"Seite 1: +{len(all_rows)}\")\n",
    "\n",
    "    # 2) Restliche Seiten p=2..N\n",
    "    for p in range(2, n_pages + 1):\n",
    "        url_p = with_page(start_url, p)\n",
    "        r = s.get(url_p, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        chunk = parse_results(r.text)\n",
    "        print(f\"Seite {p}: +{len(chunk)}\")\n",
    "        all_rows.extend(chunk)\n",
    "        time.sleep(0.8)  # höflich\n",
    "\n",
    "    # 3) Dedupe und speichern\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    # einfache Dedupe-Heuristik:\n",
    "    df[\"dedupe_key\"] = (df[\"name\"].fillna(\"\") + \"|\" + df[\"street\"].fillna(\"\") + \"|\" + df[\"postcode\"].fillna(\"\"))\n",
    "    df = df.drop_duplicates(subset=[\"dedupe_key\"]).drop(columns=[\"dedupe_key\"])\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Gespeichert: {out_csv} – {len(df)} Zeilen\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
