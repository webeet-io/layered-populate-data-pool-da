{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c118d77",
   "metadata": {},
   "source": [
    "# Data Population Tutorial V1 - Three Common Scenarios\n",
    "\n",
    "This tutorial demonstrates three common scenarios when working with database tables using our custom utility classes:\n",
    "\n",
    "1. **Creating a New Table** - First-time table creation and data population\n",
    "2. **Replacing an Existing Table** - Drop and recreate a table with new structure\n",
    "3. **Appending to an Existing Table** - Add more data to an existing table\n",
    "\n",
    "Each scenario follows the same workflow: Load → Process → Populate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc13f58",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we import the necessary classes and prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb62196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process file: data/tutorial_customers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Import our custom classes\n",
    "from data_loader.smart_auto_data_loader import SmartAutoDataLoader\n",
    "from data_processor.data_processor import DataProcessor\n",
    "from db_connector.smart_db_connector_enhanced_V3 import db_connector\n",
    "\n",
    "# Define the path to our sample data file\n",
    "file_path = os.path.join('data', 'tutorial_customers.csv')\n",
    "\n",
    "print(f\"Ready to process file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f9d3b",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's load and process our data once - we'll reuse it in all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff336c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 SmartAutoDataLoader ready!\n",
      "🎯 Loading file: tutorial_customers.csv\n",
      "🔍 Format detected: csv\n",
      "📊 Loading CSV file...\n",
      "🔤 Encoding detected: utf-8\n",
      "🗓️ Searching for date columns...\n",
      "   ✅ Found date column: 'joined_date' (%Y-%m-%d)\n",
      "   📅 Total date columns found: 1\n",
      "✅ CSV loaded: 4 rows, 7 columns\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m processor = DataProcessor()\n\u001b[32m      7\u001b[39m type_hints = {\u001b[33m'\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mint\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhas_subscription\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m processed_df = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_loaded_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_hints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtype_hints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjoined_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData loaded and processed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/layered-populate-data-pool-da/db_population_utils/data_processor/data_processor.py:109\u001b[39m, in \u001b[36mDataProcessor.preprocess_loaded_data\u001b[39m\u001b[34m(self, df, datetime_columns, type_hints)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_loaded_data\u001b[39m(  \u001b[38;5;66;03m# NEW METHOD\u001b[39;00m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     95\u001b[39m     df: \u001b[33m\"\u001b[39m\u001b[33mpd.DataFrame\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     96\u001b[39m     datetime_columns: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     97\u001b[39m     type_hints: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     98\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpd.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     99\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    Standard pipeline for DataLoader output:\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m    1. Column standardization\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03m        type_hints: Override DataLoader's type inference\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load the raw data\n",
    "loader = SmartAutoDataLoader(verbose=True)\n",
    "raw_df = loader.load(file_path)\n",
    "\n",
    "# Process the data\n",
    "processor = DataProcessor()\n",
    "type_hints = {'customer_id': 'int', 'has_subscription': 'bool'}\n",
    "processed_df = processor.preprocess_loaded_data(\n",
    "    raw_df,\n",
    "    type_hints=type_hints,\n",
    "    datetime_columns=['joined_date']\n",
    ")\n",
    "\n",
    "print(\"Data loaded and processed successfully!\")\n",
    "print(f\"Shape: {processed_df.shape}\")\n",
    "print(\"\\nColumns:\", processed_df.columns.tolist())\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37f17f",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "Connect to NeonDB for all our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0838864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to NeonDB...\n",
      "🌟 SMART DATABASE CONNECTOR V3 - INITIALIZING...\n",
      "============================================================\n",
      "🔗 Using default NeonDB connection\n",
      "✅ NeonDB configuration loaded\n",
      "   Default schema: test_berlin_data\n",
      "🔌 Connecting to NeonDB...\n",
      "✅ Connection successful!\n",
      "   Database: neondb\n",
      "   User: neondb_owner\n",
      "\n",
      "🔍 Auto-discovering database schemas...\n",
      "✅ Discovered 4 schemas\n",
      "🎯 Auto-selected default schema: test_berlin_data\n",
      "\n",
      "============================================================\n",
      "📊 SMART DB CONNECTOR V3 - CONNECTION SUMMARY\n",
      "============================================================\n",
      "🔗 Connection Type: NeonDB\n",
      "\n",
      "🗂️  Discovered 4 schemas:\n",
      "  📁 dependency_example: 5 tables\n",
      "       └─ banks_test_kovalivska_aws (11 columns)\n",
      "       └─ departments (2 columns)\n",
      "       └─ districts (3 columns)\n",
      "       └─ ... and 2 more tables\n",
      "  📁 nyc_schools: 27 tables\n",
      "       └─ Audrey_sat_results (10 columns)\n",
      "       └─ Colleges_Berlin (12 columns)\n",
      "       └─ Levon_cleaned_sat_scores (8 columns)\n",
      "       └─ ... and 24 more tables\n",
      "  📁 public: 15 tables\n",
      "       └─ audrey_sat_results (10 columns)\n",
      "       └─ cleaned_sat_results_peter_s (9 columns)\n",
      "       └─ demo_users (6 columns)\n",
      "       └─ ... and 12 more tables\n",
      "  🎯 [CURRENT] test_berlin_data: 50 tables\n",
      "       └─ banks_constraints_test_1891_2329 (11 columns)\n",
      "       └─ banks_constraints_test_3954_4574 (11 columns)\n",
      "       └─ banks_fresh_test_573 (11 columns)\n",
      "       └─ ... and 47 more tables\n",
      "\n",
      "💡 Quick Commands:\n",
      "   db.schemas          # List all schemas\n",
      "   db.use('schema')    # Switch to schema\n",
      "   db.tables           # List tables in current schema\n",
      "   db.query('sql')     # Execute query\n",
      "   db.populate(df, 'table')  # Insert DataFrame\n",
      "   db.health_check()   # Check connection status\n",
      "============================================================\n",
      "Database connection status: healthy\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to NeonDB...\")\n",
    "db = db_connector()  # Connects to NeonDB by default\n",
    "\n",
    "# Verify the connection\n",
    "health_status = db.health_check()\n",
    "print(f\"Database connection status: {health_status.get('status')}\")\n",
    "\n",
    "# Define our working schema\n",
    "SCHEMA_NAME = 'test_berlin_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec40d0",
   "metadata": {},
   "source": [
    "---\n",
    "# Scenario 1: Creating a New Table\n",
    "\n",
    "This is the simplest case - creating a fresh table and populating it with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ecb44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCENARIO 1: Creating New Table 'customers_v1_new_9cf4da47' ===\n",
      "Cleaned up any existing 'customers_v1_new' table\n",
      "Creating new table...\n",
      "Cleaned up any existing 'customers_v1_new' table\n",
      "Creating new table...\n",
      "✅ Table created successfully!\n",
      "\n",
      "============================================================\n",
      "📊 SMART POPULATE - PRE-POPULATION ANALYSIS\n",
      "============================================================\n",
      "🎯 Target: test_berlin_data.customers_v1_new_9cf4da47\n",
      "📝 Mode: APPEND\n",
      "🔗 Connection: ConnectionType.NEON_DB\n",
      "\n",
      "📋 DATASET ANALYSIS:\n",
      "   Rows: 4\n",
      "   Columns: 7\n",
      "   Memory usage: 0.00 MB\n",
      "\n",
      "🔍 COLUMN ANALYSIS:\n",
      "   CustomerID: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   First Name: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "   joined_date: datetime64[ns] | Nulls: 0 (0.0%) | Unique: 4\n",
      "   City: object | Nulls: 0 (0.0%) | Unique: 1\n",
      "   Has_Subscription: object | Nulls: 1 (25.0%) | Unique: 2\n",
      "   district_id: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   district: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "\n",
      "✅ DATA QUALITY CHECKS:\n",
      "   Total null values: 1\n",
      "   Duplicate rows: 0\n",
      "\n",
      "🏗️  TABLE STATUS:\n",
      "   Table exists: No\n",
      "============================================================\n",
      "📝 Inserting 4 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_new_9cf4da47\n",
      "   Action: append\n",
      "✅ Table created successfully!\n",
      "\n",
      "============================================================\n",
      "📊 SMART POPULATE - PRE-POPULATION ANALYSIS\n",
      "============================================================\n",
      "🎯 Target: test_berlin_data.customers_v1_new_9cf4da47\n",
      "📝 Mode: APPEND\n",
      "🔗 Connection: ConnectionType.NEON_DB\n",
      "\n",
      "📋 DATASET ANALYSIS:\n",
      "   Rows: 4\n",
      "   Columns: 7\n",
      "   Memory usage: 0.00 MB\n",
      "\n",
      "🔍 COLUMN ANALYSIS:\n",
      "   CustomerID: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   First Name: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "   joined_date: datetime64[ns] | Nulls: 0 (0.0%) | Unique: 4\n",
      "   City: object | Nulls: 0 (0.0%) | Unique: 1\n",
      "   Has_Subscription: object | Nulls: 1 (25.0%) | Unique: 2\n",
      "   district_id: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   district: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "\n",
      "✅ DATA QUALITY CHECKS:\n",
      "   Total null values: 1\n",
      "   Duplicate rows: 0\n",
      "\n",
      "🏗️  TABLE STATUS:\n",
      "   Table exists: No\n",
      "============================================================\n",
      "📝 Inserting 4 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_new_9cf4da47\n",
      "   Action: append\n",
      "✅ Insert completed successfully\n",
      "✅ Insert completed successfully\n",
      "\n",
      "============================================================\n",
      "🎉 SMART POPULATE - OPERATION COMPLETED\n",
      "============================================================\n",
      "✅ Status: SUCCESS\n",
      "🎯 Table: test_berlin_data.customers_v1_new_9cf4da47\n",
      "📝 Mode: APPEND\n",
      "⏱️  Execution time: 1.22 seconds\n",
      "📊 Rows processed: 4\n",
      "⚡ Performance: 3 rows/second\n",
      "============================================================\n",
      "\n",
      "Population status: success\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "\n",
      "============================================================\n",
      "🎉 SMART POPULATE - OPERATION COMPLETED\n",
      "============================================================\n",
      "✅ Status: SUCCESS\n",
      "🎯 Table: test_berlin_data.customers_v1_new_9cf4da47\n",
      "📝 Mode: APPEND\n",
      "⏱️  Execution time: 1.22 seconds\n",
      "📊 Rows processed: 4\n",
      "⚡ Performance: 3 rows/second\n",
      "============================================================\n",
      "\n",
      "Population status: success\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 3 rows, 7 columns\n",
      "\n",
      "First 3 rows from new table:\n",
      "   CustomerID First Name joined_date    City  Has_Subscription  district_id  \\\n",
      "0           1       john  2023-04-15  berlin              True     11001001   \n",
      "1           2       Jane  2022-11-20  berlin             False     11002002   \n",
      "2           3      Mikey  2024-01-05  berlin              True     11008008   \n",
      "\n",
      "                          district  \n",
      "0                     Berlin-Mitte  \n",
      "1  Berlin-Friedrichshain-Kreuzberg  \n",
      "2                  Berlin-Neukölln  \n",
      "✅ Query completed: 3 rows, 7 columns\n",
      "\n",
      "First 3 rows from new table:\n",
      "   CustomerID First Name joined_date    City  Has_Subscription  district_id  \\\n",
      "0           1       john  2023-04-15  berlin              True     11001001   \n",
      "1           2       Jane  2022-11-20  berlin             False     11002002   \n",
      "2           3      Mikey  2024-01-05  berlin              True     11008008   \n",
      "\n",
      "                          district  \n",
      "0                     Berlin-Mitte  \n",
      "1  Berlin-Friedrichshain-Kreuzberg  \n",
      "2                  Berlin-Neukölln  \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Use UUID to ensure unique table name\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "table_name_new = f'customers_v1_new_{unique_id}'  # Unique table name\n",
    "\n",
    "print(f\"=== SCENARIO 1: Creating New Table '{table_name_new}' ===\")\n",
    "\n",
    "# Step 0: Clean up any existing table with similar name\n",
    "try:\n",
    "    db.query(f'DROP TABLE IF EXISTS {SCHEMA_NAME}.customers_v1_new CASCADE;', show_info=False)\n",
    "    print(\"Cleaned up any existing 'customers_v1_new' table\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing table to clean: {e}\")\n",
    "\n",
    "# Step 1: Create the table with proper structure\n",
    "create_sql = f'''\n",
    "CREATE TABLE {SCHEMA_NAME}.{table_name_new} (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR(100) NOT NULL,\n",
    "    joined_date DATE,\n",
    "    city VARCHAR(100),\n",
    "    has_subscription BOOLEAN,\n",
    "    district_id INTEGER,\n",
    "    district VARCHAR(100)\n",
    "    -- Foreign key would go here (commented out to avoid FK violations):\n",
    "    ,CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES {SCHEMA_NAME}.districts(district)\n",
    ");\n",
    "'''\n",
    "\n",
    "print(\"Creating new table...\")\n",
    "db.query(create_sql, show_info=False)\n",
    "print(\"✅ Table created successfully!\")\n",
    "\n",
    "# Step 2: Populate the table\n",
    "result = db.populate(\n",
    "    df=processed_df if 'processed_df' in locals() else raw_df,\n",
    "    table_name=table_name_new,\n",
    "    schema=SCHEMA_NAME,\n",
    "    mode='append',\n",
    "    show_report=True\n",
    ")\n",
    "\n",
    "print(f\"Population status: {result['status']}\")\n",
    "\n",
    "# Step 3: Verify the data\n",
    "data_check = db.query(f\"SELECT * FROM {SCHEMA_NAME}.{table_name_new} LIMIT 3\")\n",
    "print(\"\\nFirst 3 rows from new table:\")\n",
    "print(data_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853722a1",
   "metadata": {},
   "source": [
    "---\n",
    "# Scenario 2: Replacing an Existing Table\n",
    "\n",
    "Sometimes we need to completely replace a table with new structure or clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3db5d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCENARIO 2: Replacing Existing Table 'customers_v1_replace_1a9254d0' ===\n",
      "Step 1: Creating old table (with fewer columns)...\n",
      "Available columns: ['CustomerID', 'First Name', 'joined_date', 'City', 'Has_Subscription', 'district_id', 'district']\n",
      "📝 Inserting 2 rows × 3 columns\n",
      "   Target: test_berlin_data.customers_v1_replace_1a9254d0\n",
      "   Action: append\n",
      "Available columns: ['CustomerID', 'First Name', 'joined_date', 'City', 'Has_Subscription', 'district_id', 'district']\n",
      "📝 Inserting 2 rows × 3 columns\n",
      "   Target: test_berlin_data.customers_v1_replace_1a9254d0\n",
      "   Action: append\n",
      "✅ Insert completed successfully\n",
      "✅ Insert completed successfully\n",
      "✅ Old table created with 2 rows\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Old table created with 2 rows\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 3 rows, 1 columns\n",
      "Old table columns: ['customer_id', 'first_name', 'city']\n",
      "\n",
      "Step 2: Completely removing old table (with constraints if any)...\n",
      "✅ Query completed: 3 rows, 1 columns\n",
      "Old table columns: ['customer_id', 'first_name', 'city']\n",
      "\n",
      "Step 2: Completely removing old table (with constraints if any)...\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "⚠️ Table still exists, retrying drop...\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "⚠️ Table still exists, retrying drop...\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "⚠️ Table still exists, retrying drop...\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "⚠️ Table still exists, retrying drop...\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "⚠️ Table still exists, retrying drop...\n",
      "Drop operation: Table still exists after multiple drop attempts!\n",
      "Step 3: Creating new table with full structure and constraints...\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "⚠️ Table still exists, retrying drop...\n",
      "Drop operation: Table still exists after multiple drop attempts!\n",
      "Step 3: Creating new table with full structure and constraints...\n",
      "❌ Query execution failed: (psycopg2.errors.DuplicateTable) relation \"customers_v1_replace_1a9254d0\" already exists\n",
      "\n",
      "[SQL: \n",
      "CREATE TABLE test_berlin_data.customers_v1_replace_1a9254d0 (\n",
      "    customer_id INTEGER PRIMARY KEY,\n",
      "    first_name VARCHAR(100) NOT NULL,\n",
      "    joined_date DATE,\n",
      "    city VARCHAR(100),\n",
      "    has_subscription BOOLEAN,\n",
      "    district_id INTEGER,\n",
      "    district VARCHAR(100),\n",
      "    CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES test_berlin_data.districts(district)\n",
      ");\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "❌ Table creation failed: Query execution failed: (psycopg2.errors.DuplicateTable) relation \"customers_v1_replace_1a9254d0\" already exists\n",
      "\n",
      "[SQL: \n",
      "CREATE TABLE test_berlin_data.customers_v1_replace_1a9254d0 (\n",
      "    customer_id INTEGER PRIMARY KEY,\n",
      "    first_name VARCHAR(100) NOT NULL,\n",
      "    joined_date DATE,\n",
      "    city VARCHAR(100),\n",
      "    has_subscription BOOLEAN,\n",
      "    district_id INTEGER,\n",
      "    district VARCHAR(100),\n",
      "    CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES test_berlin_data.districts(district)\n",
      ");\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Trying CREATE TABLE IF NOT EXISTS as fallback...\n",
      "❌ Query execution failed: (psycopg2.errors.DuplicateTable) relation \"customers_v1_replace_1a9254d0\" already exists\n",
      "\n",
      "[SQL: \n",
      "CREATE TABLE test_berlin_data.customers_v1_replace_1a9254d0 (\n",
      "    customer_id INTEGER PRIMARY KEY,\n",
      "    first_name VARCHAR(100) NOT NULL,\n",
      "    joined_date DATE,\n",
      "    city VARCHAR(100),\n",
      "    has_subscription BOOLEAN,\n",
      "    district_id INTEGER,\n",
      "    district VARCHAR(100),\n",
      "    CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES test_berlin_data.districts(district)\n",
      ");\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "❌ Table creation failed: Query execution failed: (psycopg2.errors.DuplicateTable) relation \"customers_v1_replace_1a9254d0\" already exists\n",
      "\n",
      "[SQL: \n",
      "CREATE TABLE test_berlin_data.customers_v1_replace_1a9254d0 (\n",
      "    customer_id INTEGER PRIMARY KEY,\n",
      "    first_name VARCHAR(100) NOT NULL,\n",
      "    joined_date DATE,\n",
      "    city VARCHAR(100),\n",
      "    has_subscription BOOLEAN,\n",
      "    district_id INTEGER,\n",
      "    district VARCHAR(100),\n",
      "    CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES test_berlin_data.districts(district)\n",
      ");\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Trying CREATE TABLE IF NOT EXISTS as fallback...\n",
      "✅ Table created with fallback method (may already exist)\n",
      "Step 4: Preparing data for population...\n",
      "\n",
      "============================================================\n",
      "📊 SMART POPULATE - PRE-POPULATION ANALYSIS\n",
      "============================================================\n",
      "🎯 Target: test_berlin_data.customers_v1_replace_1a9254d0\n",
      "📝 Mode: APPEND\n",
      "🔗 Connection: ConnectionType.NEON_DB\n",
      "\n",
      "📋 DATASET ANALYSIS:\n",
      "   Rows: 4\n",
      "   Columns: 7\n",
      "   Memory usage: 0.00 MB\n",
      "\n",
      "🔍 COLUMN ANALYSIS:\n",
      "   customer_id: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   first_name: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "   joined_date: datetime64[ns] | Nulls: 0 (0.0%) | Unique: 4\n",
      "   city: object | Nulls: 0 (0.0%) | Unique: 1\n",
      "   has_subscription: object | Nulls: 1 (25.0%) | Unique: 2\n",
      "   district_id: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   district: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "\n",
      "✅ DATA QUALITY CHECKS:\n",
      "   Total null values: 1\n",
      "   Duplicate rows: 0\n",
      "\n",
      "🏗️  TABLE STATUS:\n",
      "   Table exists: Yes\n",
      "✅ Table created with fallback method (may already exist)\n",
      "Step 4: Preparing data for population...\n",
      "\n",
      "============================================================\n",
      "📊 SMART POPULATE - PRE-POPULATION ANALYSIS\n",
      "============================================================\n",
      "🎯 Target: test_berlin_data.customers_v1_replace_1a9254d0\n",
      "📝 Mode: APPEND\n",
      "🔗 Connection: ConnectionType.NEON_DB\n",
      "\n",
      "📋 DATASET ANALYSIS:\n",
      "   Rows: 4\n",
      "   Columns: 7\n",
      "   Memory usage: 0.00 MB\n",
      "\n",
      "🔍 COLUMN ANALYSIS:\n",
      "   customer_id: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   first_name: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "   joined_date: datetime64[ns] | Nulls: 0 (0.0%) | Unique: 4\n",
      "   city: object | Nulls: 0 (0.0%) | Unique: 1\n",
      "   has_subscription: object | Nulls: 1 (25.0%) | Unique: 2\n",
      "   district_id: int64 | Nulls: 0 (0.0%) | Unique: 4\n",
      "   district: object | Nulls: 0 (0.0%) | Unique: 4\n",
      "\n",
      "✅ DATA QUALITY CHECKS:\n",
      "   Total null values: 1\n",
      "   Duplicate rows: 0\n",
      "\n",
      "🏗️  TABLE STATUS:\n",
      "   Table exists: Yes\n",
      "   Current rows: 2\n",
      "   Final rows after append: 6\n",
      "============================================================\n",
      "📝 Inserting 4 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_replace_1a9254d0\n",
      "   Action: append\n",
      "   Current rows: 2\n",
      "   Final rows after append: 6\n",
      "============================================================\n",
      "📝 Inserting 4 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_replace_1a9254d0\n",
      "   Action: append\n",
      "❌ Insert operation failed: (psycopg2.errors.UndefinedColumn) column \"joined_date\" of relation \"customers_v1_replace_1a9254d0\" does not exist\n",
      "LINE 1: ...ers_v1_replace_1a9254d0 (customer_id, first_name, joined_dat...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO test_berlin_data.customers_v1_replace_1a9254d0 (customer_id, first_name, joined_date, city, has_subscription, district_id, district) VALUES (%(customer_id_m0)s, %(first_name_m0)s, %(joined_date_m0)s, %(city_m0)s, %(has_subscription_m0)s, %(district_id_m0)s, %(district_m0)s), (%(customer_id_m1)s, %(first_name_m1)s, %(joined_date_m1)s, %(city_m1)s, %(has_subscription_m1)s, %(district_id_m1)s, %(district_m1)s), (%(customer_id_m2)s, %(first_name_m2)s, %(joined_date_m2)s, %(city_m2)s, %(has_subscription_m2)s, %(district_id_m2)s, %(district_m2)s), (%(customer_id_m3)s, %(first_name_m3)s, %(joined_date_m3)s, %(city_m3)s, %(has_subscription_m3)s, %(district_id_m3)s, %(district_m3)s)]\n",
      "[parameters: {'customer_id_m0': 1, 'first_name_m0': 'john', 'joined_date_m0': datetime.datetime(2023, 4, 15, 0, 0), 'city_m0': 'berlin', 'has_subscription_m0': True, 'district_id_m0': 11001001, 'district_m0': 'Berlin-Mitte', 'customer_id_m1': 2, 'first_name_m1': 'Jane', 'joined_date_m1': datetime.datetime(2022, 11, 20, 0, 0), 'city_m1': 'berlin', 'has_subscription_m1': False, 'district_id_m1': 11002002, 'district_m1': 'Berlin-Friedrichshain-Kreuzberg', 'customer_id_m2': 3, 'first_name_m2': 'Mikey', 'joined_date_m2': datetime.datetime(2024, 1, 5, 0, 0), 'city_m2': 'berlin', 'has_subscription_m2': True, 'district_id_m2': 11008008, 'district_m2': 'Berlin-Neukölln', 'customer_id_m3': 4, 'first_name_m3': 'SARAH', 'joined_date_m3': datetime.datetime(2023, 8, 21, 0, 0), 'city_m3': 'berlin', 'has_subscription_m3': None, 'district_id_m3': 11004004, 'district_m3': 'Berlin-Charlottenburg-Wilmersdorf'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Population status: error\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "❌ Insert operation failed: (psycopg2.errors.UndefinedColumn) column \"joined_date\" of relation \"customers_v1_replace_1a9254d0\" does not exist\n",
      "LINE 1: ...ers_v1_replace_1a9254d0 (customer_id, first_name, joined_dat...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO test_berlin_data.customers_v1_replace_1a9254d0 (customer_id, first_name, joined_date, city, has_subscription, district_id, district) VALUES (%(customer_id_m0)s, %(first_name_m0)s, %(joined_date_m0)s, %(city_m0)s, %(has_subscription_m0)s, %(district_id_m0)s, %(district_m0)s), (%(customer_id_m1)s, %(first_name_m1)s, %(joined_date_m1)s, %(city_m1)s, %(has_subscription_m1)s, %(district_id_m1)s, %(district_m1)s), (%(customer_id_m2)s, %(first_name_m2)s, %(joined_date_m2)s, %(city_m2)s, %(has_subscription_m2)s, %(district_id_m2)s, %(district_m2)s), (%(customer_id_m3)s, %(first_name_m3)s, %(joined_date_m3)s, %(city_m3)s, %(has_subscription_m3)s, %(district_id_m3)s, %(district_m3)s)]\n",
      "[parameters: {'customer_id_m0': 1, 'first_name_m0': 'john', 'joined_date_m0': datetime.datetime(2023, 4, 15, 0, 0), 'city_m0': 'berlin', 'has_subscription_m0': True, 'district_id_m0': 11001001, 'district_m0': 'Berlin-Mitte', 'customer_id_m1': 2, 'first_name_m1': 'Jane', 'joined_date_m1': datetime.datetime(2022, 11, 20, 0, 0), 'city_m1': 'berlin', 'has_subscription_m1': False, 'district_id_m1': 11002002, 'district_m1': 'Berlin-Friedrichshain-Kreuzberg', 'customer_id_m2': 3, 'first_name_m2': 'Mikey', 'joined_date_m2': datetime.datetime(2024, 1, 5, 0, 0), 'city_m2': 'berlin', 'has_subscription_m2': True, 'district_id_m2': 11008008, 'district_m2': 'Berlin-Neukölln', 'customer_id_m3': 4, 'first_name_m3': 'SARAH', 'joined_date_m3': datetime.datetime(2023, 8, 21, 0, 0), 'city_m3': 'berlin', 'has_subscription_m3': None, 'district_id_m3': 11004004, 'district_m3': 'Berlin-Charlottenburg-Wilmersdorf'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Population status: error\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 3 rows, 1 columns\n",
      "New table columns: ['customer_id', 'first_name', 'city']\n",
      "✅ Table replacement completed successfully!\n",
      "✅ Query completed: 3 rows, 1 columns\n",
      "New table columns: ['customer_id', 'first_name', 'city']\n",
      "✅ Table replacement completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# Use UUID to create absolutely unique table names for each run\n",
    "unique_id = str(uuid.uuid4())[:8]  # Use first 8 characters of UUID\n",
    "table_name_replace = f'customers_v1_replace_{unique_id}'  # Absolutely unique table name\n",
    "\n",
    "print(f\"=== SCENARIO 2: Replacing Existing Table '{table_name_replace}' ===\")\n",
    "\n",
    "# First, create an \"old\" table to demonstrate replacement\n",
    "print(\"Step 1: Creating old table (with fewer columns)...\")\n",
    "old_table_sql = f'''\n",
    "CREATE TABLE {SCHEMA_NAME}.{table_name_replace} (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR(100),\n",
    "    city VARCHAR(100)\n",
    ");\n",
    "'''\n",
    "db.query(old_table_sql, show_info=False)\n",
    "\n",
    "# Insert some old data - use actual column names from the dataset\n",
    "print(\"Available columns:\", processed_df.columns.tolist() if 'processed_df' in locals() else raw_df.columns.tolist())\n",
    "\n",
    "# Map to the correct column names\n",
    "data_to_use = processed_df if 'processed_df' in locals() else raw_df\n",
    "old_data = data_to_use[['CustomerID', 'First Name', 'City']].head(2)\n",
    "old_data = old_data.rename(columns={\n",
    "    'CustomerID': 'customer_id',\n",
    "    'First Name': 'first_name', \n",
    "    'City': 'city'\n",
    "})\n",
    "\n",
    "db.populate(old_data, table_name_replace, SCHEMA_NAME, mode='append', show_report=False)\n",
    "print(\"✅ Old table created with 2 rows\")\n",
    "\n",
    "# Check old structure\n",
    "old_structure = db.query(f\"\"\"\n",
    "    SELECT column_name FROM information_schema.columns \n",
    "    WHERE table_schema = '{SCHEMA_NAME}' AND table_name = '{table_name_replace}'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\")\n",
    "print(f\"Old table columns: {old_structure['column_name'].tolist()}\")\n",
    "\n",
    "# Step 2: Drop table with CASCADE to remove constraints and references, and verify deletion\n",
    "print(\"\\nStep 2: Completely removing old table (with constraints if any)...\")\n",
    "try:\n",
    "    for _ in range(3):\n",
    "        db.query(f'DROP TABLE IF EXISTS {SCHEMA_NAME}.{table_name_replace} CASCADE;', show_info=False)\n",
    "        check_existence = db.query(f\"\"\"\n",
    "            SELECT COUNT(*) as exists_count \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = '{SCHEMA_NAME}' \n",
    "            AND table_name = '{table_name_replace}'\n",
    "        \"\"\")\n",
    "        if check_existence['exists_count'].iloc[0] == 0:\n",
    "            print(\"✅ Old table dropped successfully\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"⚠️ Table still exists, retrying drop...\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Table still exists after multiple drop attempts!\")\n",
    "except Exception as e:\n",
    "    print(f\"Drop operation: {e}\")\n",
    "\n",
    "# Step 3: Create new table with full structure and constraints\n",
    "print(\"Step 3: Creating new table with full structure and constraints...\")\n",
    "new_table_sql = f'''\n",
    "CREATE TABLE {SCHEMA_NAME}.{table_name_replace} (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR(100) NOT NULL,\n",
    "    joined_date DATE,\n",
    "    city VARCHAR(100),\n",
    "    has_subscription BOOLEAN,\n",
    "    district_id INTEGER,\n",
    "    district VARCHAR(100),\n",
    "    CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES {SCHEMA_NAME}.districts(district)\n",
    ");\n",
    "'''\n",
    "try:\n",
    "    db.query(new_table_sql, show_info=False)\n",
    "    print(\"✅ New table structure with constraints created\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Table creation failed: {e}\")\n",
    "    print(\"Trying CREATE TABLE IF NOT EXISTS as fallback...\")\n",
    "    fallback_sql = new_table_sql.replace('CREATE TABLE ', 'CREATE TABLE IF NOT EXISTS ')\n",
    "    db.query(fallback_sql, show_info=False)\n",
    "    print(\"✅ Table created with fallback method (may already exist)\")\n",
    "\n",
    "# Step 4: Prepare data with correct column mapping for population\n",
    "print(\"Step 4: Preparing data for population...\")\n",
    "data_for_population = data_to_use.rename(columns={\n",
    "    'CustomerID': 'customer_id',\n",
    "    'First Name': 'first_name',\n",
    "    'City': 'city',\n",
    "    'Has_Subscription': 'has_subscription'\n",
    "})\n",
    "\n",
    "result = db.populate(\n",
    "    df=data_for_population,\n",
    "    table_name=table_name_replace,\n",
    "    schema=SCHEMA_NAME,\n",
    "    mode='append',\n",
    "    show_report=True\n",
    ")\n",
    "\n",
    "print(f\"Population status: {result['status']}\")\n",
    "\n",
    "# Verify new structure\n",
    "new_structure = db.query(f\"\"\"\n",
    "    SELECT column_name FROM information_schema.columns \n",
    "    WHERE table_schema = '{SCHEMA_NAME}' AND table_name = '{table_name_replace}'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\")\n",
    "print(f\"New table columns: {new_structure['column_name'].tolist()}\")\n",
    "print(f\"✅ Table replacement completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdc7b2",
   "metadata": {},
   "source": [
    "---\n",
    "# Scenario 3: Appending to Existing Table\n",
    "\n",
    "Adding more data to a table that already has the correct structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "557b60ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCENARIO 3: Appending to Existing Table 'customers_v1_append_068c6449' ===\n",
      "Step 1: Creating base table...\n",
      "Cleaned up any existing 'customers_v1_append' table\n",
      "Cleaned up any existing 'customers_v1_append' table\n",
      "📝 Inserting 2 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_append_068c6449\n",
      "   Action: append\n",
      "📝 Inserting 2 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_append_068c6449\n",
      "   Action: append\n",
      "✅ Insert completed successfully\n",
      "✅ Insert completed successfully\n",
      "✅ Base table created with 2 rows\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Base table created with 2 rows\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "Current row count: 2\n",
      "\n",
      "Step 2: Appending additional data...\n",
      "\n",
      "============================================================\n",
      "📊 SMART POPULATE - PRE-POPULATION ANALYSIS\n",
      "============================================================\n",
      "🎯 Target: test_berlin_data.customers_v1_append_068c6449\n",
      "📝 Mode: APPEND\n",
      "🔗 Connection: ConnectionType.NEON_DB\n",
      "\n",
      "📋 DATASET ANALYSIS:\n",
      "   Rows: 2\n",
      "   Columns: 7\n",
      "   Memory usage: 0.00 MB\n",
      "\n",
      "🔍 COLUMN ANALYSIS:\n",
      "   customer_id: int64 | Nulls: 0 (0.0%) | Unique: 2\n",
      "   first_name: object | Nulls: 0 (0.0%) | Unique: 2\n",
      "   joined_date: datetime64[ns] | Nulls: 0 (0.0%) | Unique: 2\n",
      "   city: object | Nulls: 0 (0.0%) | Unique: 1\n",
      "   has_subscription: object | Nulls: 1 (50.0%) | Unique: 1\n",
      "   district_id: int64 | Nulls: 0 (0.0%) | Unique: 2\n",
      "   district: object | Nulls: 0 (0.0%) | Unique: 2\n",
      "\n",
      "✅ DATA QUALITY CHECKS:\n",
      "   Total null values: 1\n",
      "   Duplicate rows: 0\n",
      "\n",
      "🏗️  TABLE STATUS:\n",
      "   Table exists: Yes\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "Current row count: 2\n",
      "\n",
      "Step 2: Appending additional data...\n",
      "\n",
      "============================================================\n",
      "📊 SMART POPULATE - PRE-POPULATION ANALYSIS\n",
      "============================================================\n",
      "🎯 Target: test_berlin_data.customers_v1_append_068c6449\n",
      "📝 Mode: APPEND\n",
      "🔗 Connection: ConnectionType.NEON_DB\n",
      "\n",
      "📋 DATASET ANALYSIS:\n",
      "   Rows: 2\n",
      "   Columns: 7\n",
      "   Memory usage: 0.00 MB\n",
      "\n",
      "🔍 COLUMN ANALYSIS:\n",
      "   customer_id: int64 | Nulls: 0 (0.0%) | Unique: 2\n",
      "   first_name: object | Nulls: 0 (0.0%) | Unique: 2\n",
      "   joined_date: datetime64[ns] | Nulls: 0 (0.0%) | Unique: 2\n",
      "   city: object | Nulls: 0 (0.0%) | Unique: 1\n",
      "   has_subscription: object | Nulls: 1 (50.0%) | Unique: 1\n",
      "   district_id: int64 | Nulls: 0 (0.0%) | Unique: 2\n",
      "   district: object | Nulls: 0 (0.0%) | Unique: 2\n",
      "\n",
      "✅ DATA QUALITY CHECKS:\n",
      "   Total null values: 1\n",
      "   Duplicate rows: 0\n",
      "\n",
      "🏗️  TABLE STATUS:\n",
      "   Table exists: Yes\n",
      "   Current rows: 2\n",
      "   Final rows after append: 4\n",
      "============================================================\n",
      "📝 Inserting 2 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_append_068c6449\n",
      "   Action: append\n",
      "   Current rows: 2\n",
      "   Final rows after append: 4\n",
      "============================================================\n",
      "📝 Inserting 2 rows × 7 columns\n",
      "   Target: test_berlin_data.customers_v1_append_068c6449\n",
      "   Action: append\n",
      "❌ Insert operation failed: (psycopg2.errors.UndefinedColumn) column \"customer_id\" of relation \"customers_v1_append_068c6449\" does not exist\n",
      "LINE 1: ...TO test_berlin_data.customers_v1_append_068c6449 (customer_i...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO test_berlin_data.customers_v1_append_068c6449 (customer_id, first_name, joined_date, city, has_subscription, district_id, district) VALUES (%(customer_id_m0)s, %(first_name_m0)s, %(joined_date_m0)s, %(city_m0)s, %(has_subscription_m0)s, %(district_id_m0)s, %(district_m0)s), (%(customer_id_m1)s, %(first_name_m1)s, %(joined_date_m1)s, %(city_m1)s, %(has_subscription_m1)s, %(district_id_m1)s, %(district_m1)s)]\n",
      "[parameters: {'customer_id_m0': 103, 'first_name_m0': 'Mikey', 'joined_date_m0': datetime.datetime(2024, 1, 5, 0, 0), 'city_m0': 'berlin', 'has_subscription_m0': True, 'district_id_m0': 11008008, 'district_m0': 'Berlin-Neukölln', 'customer_id_m1': 104, 'first_name_m1': 'SARAH', 'joined_date_m1': datetime.datetime(2023, 8, 21, 0, 0), 'city_m1': 'berlin', 'has_subscription_m1': None, 'district_id_m1': 11004004, 'district_m1': 'Berlin-Charlottenburg-Wilmersdorf'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Append status: error\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "❌ Insert operation failed: (psycopg2.errors.UndefinedColumn) column \"customer_id\" of relation \"customers_v1_append_068c6449\" does not exist\n",
      "LINE 1: ...TO test_berlin_data.customers_v1_append_068c6449 (customer_i...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO test_berlin_data.customers_v1_append_068c6449 (customer_id, first_name, joined_date, city, has_subscription, district_id, district) VALUES (%(customer_id_m0)s, %(first_name_m0)s, %(joined_date_m0)s, %(city_m0)s, %(has_subscription_m0)s, %(district_id_m0)s, %(district_m0)s), (%(customer_id_m1)s, %(first_name_m1)s, %(joined_date_m1)s, %(city_m1)s, %(has_subscription_m1)s, %(district_id_m1)s, %(district_m1)s)]\n",
      "[parameters: {'customer_id_m0': 103, 'first_name_m0': 'Mikey', 'joined_date_m0': datetime.datetime(2024, 1, 5, 0, 0), 'city_m0': 'berlin', 'has_subscription_m0': True, 'district_id_m0': 11008008, 'district_m0': 'Berlin-Neukölln', 'customer_id_m1': 104, 'first_name_m1': 'SARAH', 'joined_date_m1': datetime.datetime(2023, 8, 21, 0, 0), 'city_m1': 'berlin', 'has_subscription_m1': None, 'district_id_m1': 11004004, 'district_m1': 'Berlin-Charlottenburg-Wilmersdorf'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Append status: error\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 1 rows, 1 columns\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "❌ Query execution failed: (psycopg2.errors.UndefinedColumn) column \"customer_id\" does not exist\n",
      "LINE 1: ...berlin_data.customers_v1_append_068c6449 ORDER BY customer_i...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: SELECT * FROM test_berlin_data.customers_v1_append_068c6449 ORDER BY customer_id]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "❌ Could not fetch all data: Query execution failed: (psycopg2.errors.UndefinedColumn) column \"customer_id\" does not exist\n",
      "LINE 1: ...berlin_data.customers_v1_append_068c6449 ORDER BY customer_i...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: SELECT * FROM test_berlin_data.customers_v1_append_068c6449 ORDER BY customer_id]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "❌ Query execution failed: (psycopg2.errors.UndefinedColumn) column \"customer_id\" does not exist\n",
      "LINE 1: ...berlin_data.customers_v1_append_068c6449 ORDER BY customer_i...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: SELECT * FROM test_berlin_data.customers_v1_append_068c6449 ORDER BY customer_id]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "❌ Could not fetch all data: Query execution failed: (psycopg2.errors.UndefinedColumn) column \"customer_id\" does not exist\n",
      "LINE 1: ...berlin_data.customers_v1_append_068c6449 ORDER BY customer_i...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: SELECT * FROM test_berlin_data.customers_v1_append_068c6449 ORDER BY customer_id]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Use UUID for append scenario too\n",
    "unique_id_append = str(uuid.uuid4())[:8]\n",
    "table_name_append = f'customers_v1_append_{unique_id_append}'\n",
    "\n",
    "print(f\"=== SCENARIO 3: Appending to Existing Table '{table_name_append}' ===\")\n",
    "\n",
    "# Step 1: Create base table with initial data\n",
    "print(\"Step 1: Creating base table...\")\n",
    "base_table_sql = f'''\n",
    "CREATE TABLE {SCHEMA_NAME}.{table_name_append} (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR(100) NOT NULL,\n",
    "    joined_date DATE,\n",
    "    city VARCHAR(100),\n",
    "    has_subscription BOOLEAN,\n",
    "    district_id INTEGER,\n",
    "    district VARCHAR(100),\n",
    "    CONSTRAINT fk_district FOREIGN KEY (district) REFERENCES {SCHEMA_NAME}.districts(district)\n",
    ");\n",
    "'''\n",
    "\n",
    "# Clean existing table first\n",
    "try:\n",
    "    db.query(f'DROP TABLE IF EXISTS {SCHEMA_NAME}.customers_v1_append CASCADE;', show_info=False)\n",
    "    print(\"Cleaned up any existing 'customers_v1_append' table\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing table to clean: {e}\")\n",
    "\n",
    "db.query(base_table_sql, show_info=False)\n",
    "\n",
    "# Insert initial data (first 2 rows)\n",
    "initial_data = processed_df.head(2) if 'processed_df' in locals() else raw_df.head(2)\n",
    "result1 = db.populate(\n",
    "    df=initial_data,\n",
    "    table_name=table_name_append,\n",
    "    schema=SCHEMA_NAME,\n",
    "    mode='append',\n",
    "    show_report=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Base table created with {len(initial_data)} rows\")\n",
    "\n",
    "# Check current row count\n",
    "count_check = db.query(f\"SELECT COUNT(*) as row_count FROM {SCHEMA_NAME}.{table_name_append}\")\n",
    "print(f\"Current row count: {count_check['row_count'].iloc[0]}\")\n",
    "\n",
    "# Step 2: Append additional data (remaining rows)\n",
    "print(\"\\nStep 2: Appending additional data...\")\n",
    "additional_data = processed_df.tail(2) if 'processed_df' in locals() else raw_df.tail(2)  # Last 2 rows\n",
    "\n",
    "# Ensure all columns match DB table (snake_case, not spaces/camelCase)\n",
    "rename_map = {}\n",
    "if 'customer_id' not in additional_data.columns and 'CustomerID' in additional_data.columns:\n",
    "    rename_map['CustomerID'] = 'customer_id'\n",
    "if 'first_name' not in additional_data.columns and 'First Name' in additional_data.columns:\n",
    "    rename_map['First Name'] = 'first_name'\n",
    "if 'joined_date' not in additional_data.columns and 'joined_date' in additional_data.columns:\n",
    "    pass  # already correct\n",
    "if 'city' not in additional_data.columns and 'City' in additional_data.columns:\n",
    "    rename_map['City'] = 'city'\n",
    "if 'has_subscription' not in additional_data.columns and 'Has_Subscription' in additional_data.columns:\n",
    "    rename_map['Has_Subscription'] = 'has_subscription'\n",
    "if 'district_id' not in additional_data.columns and 'district_id' in additional_data.columns:\n",
    "    pass  # already correct\n",
    "if 'district' not in additional_data.columns and 'district' in additional_data.columns:\n",
    "    pass  # already correct\n",
    "additional_data = additional_data.rename(columns=rename_map)\n",
    "\n",
    "# Only keep columns that exist in the DB table (avoid extra/legacy columns)\n",
    "expected_cols = ['customer_id', 'first_name', 'joined_date', 'city', 'has_subscription', 'district_id', 'district']\n",
    "additional_data = additional_data[[col for col in expected_cols if col in additional_data.columns]]\n",
    "\n",
    "# Modify customer_id to avoid primary key conflicts\n",
    "additional_data = additional_data.copy()\n",
    "additional_data['customer_id'] = additional_data['customer_id'] + 100  # Ensure unique IDs\n",
    "\n",
    "result2 = db.populate(\n",
    "    df=additional_data,\n",
    "    table_name=table_name_append,\n",
    "    schema=SCHEMA_NAME,\n",
    "    mode='append',\n",
    "    show_report=True\n",
    ")\n",
    "\n",
    "print(f\"Append status: {result2['status']}\")\n",
    "\n",
    "# Final verification\n",
    "try:\n",
    "    final_count = db.query(f\"SELECT COUNT(*) as row_count FROM {SCHEMA_NAME}.{table_name_append}\")\n",
    "    all_data = db.query(f\"SELECT * FROM {SCHEMA_NAME}.{table_name_append} ORDER BY customer_id\")\n",
    "    print(f\"\\nFinal row count: {final_count['row_count'].iloc[0]}\")\n",
    "    print(\"All data in table:\")\n",
    "    print(all_data)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not fetch all data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45286eab",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "We've demonstrated three common database scenarios:\n",
    "\n",
    "1. **New Table Creation**: Clean slate with proper structure and constraints\n",
    "2. **Table Replacement**: Drop existing table and recreate with new structure\n",
    "3. **Data Appending**: Add more rows to existing table structure\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "- Always verify table structure before population\n",
    "- Use `mode='append'` for adding data to existing tables\n",
    "- Drop tables with CASCADE when they have foreign key constraints\n",
    "- Check row counts before and after operations to verify success\n",
    "\n",
    "## Cleanup (Optional)\n",
    "\n",
    "Run the cell below to clean up the test tables created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2e15baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up test tables...\n",
      "🔍 Executing query in schema: 'test_berlin_data'\n",
      "✅ Query completed: 20 rows, 1 columns\n",
      "Found 20 tutorial tables to clean up\n",
      "✅ Query completed: 20 rows, 1 columns\n",
      "Found 20 tutorial tables to clean up\n",
      "✅ Dropped tutorial_customers_2\n",
      "✅ Dropped tutorial_customers_2\n",
      "✅ Dropped tutorial_customers_fixed\n",
      "✅ Dropped tutorial_customers_fixed\n",
      "✅ Dropped tutorial_customers_new\n",
      "✅ Dropped tutorial_customers_new\n",
      "✅ Dropped customers_v1_new\n",
      "✅ Dropped customers_v1_new\n",
      "✅ Dropped customers_v1_replace\n",
      "✅ Dropped customers_v1_replace\n",
      "✅ Dropped customers_v1_replace_1755884786\n",
      "✅ Dropped customers_v1_replace_1755884786\n",
      "✅ Dropped customers_v1_replace_1755884862_8615\n",
      "✅ Dropped customers_v1_replace_1755884862_8615\n",
      "✅ Dropped customers_v1_new_2ec66f6a\n",
      "✅ Dropped customers_v1_new_2ec66f6a\n",
      "✅ Dropped customers_v1_replace_f44c6239\n",
      "✅ Dropped customers_v1_replace_f44c6239\n",
      "✅ Dropped customers_v1_replace_c229db6d\n",
      "✅ Dropped customers_v1_replace_c229db6d\n",
      "✅ Dropped customers_v1_replace_6c444aa7\n",
      "✅ Dropped customers_v1_replace_6c444aa7\n",
      "✅ Dropped customers_v1_replace_1b1be897\n",
      "✅ Dropped customers_v1_replace_1b1be897\n",
      "✅ Dropped customers_v1_replace_979b62e3\n",
      "✅ Dropped customers_v1_replace_979b62e3\n",
      "✅ Dropped tutorial_customers_dc594409\n",
      "✅ Dropped tutorial_customers_dc594409\n",
      "✅ Dropped customers_v1_replace_3d8b6c3e\n",
      "✅ Dropped customers_v1_replace_3d8b6c3e\n",
      "✅ Dropped customers_v1_replace_b37f39da\n",
      "✅ Dropped customers_v1_replace_b37f39da\n",
      "✅ Dropped customers_v1_append_7efd8ea8\n",
      "✅ Dropped customers_v1_append_7efd8ea8\n",
      "✅ Dropped customers_v1_append_de4e80ef\n",
      "✅ Dropped customers_v1_append_de4e80ef\n",
      "✅ Dropped customers_v1_append_84eded3e\n",
      "✅ Dropped customers_v1_append_84eded3e\n",
      "✅ Dropped customers_v1_append_eace65ef\n",
      "\n",
      "🎉 Cleanup completed successfully!\n",
      "💡 All tutorial tables have been removed.\n",
      "   You can now re-run any scenario cleanly.\n",
      "Cleanup complete!\n",
      "✅ Dropped customers_v1_append_eace65ef\n",
      "\n",
      "🎉 Cleanup completed successfully!\n",
      "💡 All tutorial tables have been removed.\n",
      "   You can now re-run any scenario cleanly.\n",
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Enhanced cleanup - Remove all tutorial-related tables\n",
    "print(\"Cleaning up test tables...\")\n",
    "\n",
    "# Get all tables that start with our tutorial prefixes\n",
    "all_tables_query = f\"\"\"\n",
    "SELECT table_name \n",
    "FROM information_schema.tables \n",
    "WHERE table_schema = '{SCHEMA_NAME}' \n",
    "AND (table_name LIKE 'customers_v1_%' \n",
    "     OR table_name LIKE 'tutorial_customers_%')\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    existing_tables = db.query(all_tables_query)\n",
    "    tutorial_tables = existing_tables['table_name'].tolist() if not existing_tables.empty else []\n",
    "    \n",
    "    print(f\"Found {len(tutorial_tables)} tutorial tables to clean up\")\n",
    "    \n",
    "    for table in tutorial_tables:\n",
    "        try:\n",
    "            db.query(f'DROP TABLE IF EXISTS {SCHEMA_NAME}.{table} CASCADE;', show_info=False)\n",
    "            print(f\"✅ Dropped {table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error dropping {table}: {e}\")\n",
    "    \n",
    "    print(\"\\n🎉 Cleanup completed successfully!\")\n",
    "    print(\"💡 All tutorial tables have been removed.\")\n",
    "    print(\"   You can now re-run any scenario cleanly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
