{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05603311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added parent directory to path\n",
      "Current working directory: /Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data_loader/test\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Simple path fix - go up one level to data_loader directory\n",
    "sys.path.append('../')\n",
    "\n",
    "print(f\"Added parent directory to path\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903e40e",
   "metadata": {},
   "source": [
    "# CSV Loading TEST SmartAutoDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a2da6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: test.csv\n",
      "ğŸ” Format detected: csv\n",
      "ğŸ“Š Loading CSV file...\n",
      "ğŸ”¤ Encoding detected: utf-8\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… CSV loaded: 3263 rows, 4 columns\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: Check which methods are missing\n",
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "\n",
    "\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/test.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df06df82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1195a",
   "metadata": {},
   "source": [
    "# Excel Loading TEST SmartAutoDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03ca09a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: statistischer-bericht-auslaend-bevoelkerung-2010200247005.xlsx\n",
      "ğŸ” Format detected: excel\n",
      "ğŸ“ˆ Loading Excel file...\n",
      "   ğŸ“‹ Available sheets: ['Titel', 'InhaltsÃ¼bersicht', 'GENESIS-Online', 'Impressum', 'Informationen zur Statistik', '12521-01', '12521-02', '12521-03', '12521-04', '12521-05', '12521-06', '12521-07', '12521-08', '12521-09', '12521-10', '12521-11', '12521-12', '12521-13', '12521-14', '12521-15', '12521-16', 'ErlÃ¤uterung_zu_CSV-Tabellen', 'csv-12521-01', 'csv-12521-02', 'csv-12521-03', 'csv-12521-04', 'csv-12521-05', 'csv-12521-06', 'csv-12521-07', 'csv-12521-08', 'csv-12521-09', 'csv-12521-10', 'csv-12521-11', 'csv-12521-12', 'csv-12521-13', 'csv-12521-14', 'csv-12521-15', 'csv-12521-16']\n",
      "   âœ… Selected sheet: 'csv-12521-15'\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   âœ… Found date column: 'Stichtag' (%d.%m.%Y)\n",
      "   ğŸ“… Total date columns found: 1\n",
      "âœ… Excel loaded: 1309 rows, 24 columns\n",
      "   ğŸ“Š Column names: ['Statistik_Code', 'Statistik_Label', 'Stichtag', 'Region', 'AGS_Kreis', 'Geschlecht', 'Bestand_am_31_12_2023_Anzahl', 'Zugaenge_in_2024_Insgesamt_Anzahl', 'Erstzuzug_aus_dem_Ausland_in_2024_Anzahl', 'Wiederzuzug_aus_dem_Ausland_in_2024_Anzahl', 'Geburten_in_2024_Anzahl', 'Zuzug_im_Inland_in_2024_Anzahl', 'Abgaenge_in_2024_Insgesamt_Anzahl', 'Fortzuege_ins_Ausland_in_2024_Anzahl', 'Abmeldungen_ins_Ausland_in_2024_Anzahl', 'Tod_in_2024_Anzahl', 'Registerloeschung_in_2024_Anzahl', 'Fortzug_im_Inland_in_2024_Anzahl', 'Geburtenueberschuss_in_2024_Anzahl', 'Nettozuwanderung_aus_Ausland_in_2024_Anzahl', 'Nettozuwanderung_aus_Inland_in_2024_Anzahl', 'Bestand_am_31_12_2024_Anzahl', 'Nachrichtlich_Zugaenge_in_2024_Anzahl', 'Nachrichtlich_Abgaenge_in_2024_Anzahl']\n",
      "  Statistik_Code      Statistik_Label   Stichtag  \\\n",
      "0          12521  Auslaenderstatistik 2024-12-31   \n",
      "1          12521  Auslaenderstatistik 2024-12-31   \n",
      "2          12521  Auslaenderstatistik 2024-12-31   \n",
      "3          12521  Auslaenderstatistik 2024-12-31   \n",
      "4          12521  Auslaenderstatistik 2024-12-31   \n",
      "\n",
      "                                              Region  \\\n",
      "0  Deutschland                                   ...   \n",
      "1  Deutschland                                   ...   \n",
      "2  Deutschland                                   ...   \n",
      "3  Schleswig-Holstein                            ...   \n",
      "4  Schleswig-Holstein                            ...   \n",
      "\n",
      "                                           AGS_Kreis   Geschlecht  \\\n",
      "0  Kein_AGS                                      ...  Insgesamt     \n",
      "1  Kein_AGS                                      ...  Maennlich     \n",
      "2  Kein_AGS                                      ...  Weiblich      \n",
      "3  Kein_AGS                                      ...  Insgesamt     \n",
      "4  Kein_AGS                                      ...  Maennlich     \n",
      "\n",
      "   Bestand_am_31_12_2023_Anzahl  Zugaenge_in_2024_Insgesamt_Anzahl  \\\n",
      "0                    13895865.0                          1285655.0   \n",
      "1                     7305105.0                           728825.0   \n",
      "2                     6590755.0                           556835.0   \n",
      "3                      346690.0                            54800.0   \n",
      "4                      181985.0                            31310.0   \n",
      "\n",
      "   Erstzuzug_aus_dem_Ausland_in_2024_Anzahl  \\\n",
      "0                                  972850.0   \n",
      "1                                  539235.0   \n",
      "2                                  433610.0   \n",
      "3                                   28625.0   \n",
      "4                                   15835.0   \n",
      "\n",
      "   Wiederzuzug_aus_dem_Ausland_in_2024_Anzahl  ...  \\\n",
      "0                                    205350.0  ...   \n",
      "1                                    134420.0  ...   \n",
      "2                                     70930.0  ...   \n",
      "3                                      4835.0  ...   \n",
      "4                                      3115.0  ...   \n",
      "\n",
      "   Abmeldungen_ins_Ausland_in_2024_Anzahl Tod_in_2024_Anzahl  \\\n",
      "0                                324415.0              46605   \n",
      "1                                219910.0              27440   \n",
      "2                                104505.0              19160   \n",
      "3                                  9155.0                970   \n",
      "4                                  6250.0                525   \n",
      "\n",
      "   Registerloeschung_in_2024_Anzahl  Fortzug_im_Inland_in_2024_Anzahl  \\\n",
      "0                          354780.0                                 .   \n",
      "1                          188065.0                                 .   \n",
      "2                          166720.0                                 .   \n",
      "3                           10830.0                             19485   \n",
      "4                            5845.0                             11215   \n",
      "\n",
      "   Geburtenueberschuss_in_2024_Anzahl  \\\n",
      "0                             60850.0   \n",
      "1                             27725.0   \n",
      "2                             33130.0   \n",
      "3                              2115.0   \n",
      "4                              1070.0   \n",
      "\n",
      "  Nettozuwanderung_aus_Ausland_in_2024_Anzahl  \\\n",
      "0                                    459705.0   \n",
      "1                                    232455.0   \n",
      "2                                    227245.0   \n",
      "3                                     13920.0   \n",
      "4                                      7030.0   \n",
      "\n",
      "   Nettozuwanderung_aus_Inland_in_2024_Anzahl Bestand_am_31_12_2024_Anzahl  \\\n",
      "0                                           .                   14061640.0   \n",
      "1                                           .                    7377225.0   \n",
      "2                                           .                    6684415.0   \n",
      "3                                       -1235                     350665.0   \n",
      "4                                        -450                     183790.0   \n",
      "\n",
      "   Nachrichtlich_Zugaenge_in_2024_Anzahl  \\\n",
      "0                               226800.0   \n",
      "1                               128280.0   \n",
      "2                                98520.0   \n",
      "3                                 5540.0   \n",
      "4                                 3095.0   \n",
      "\n",
      "   Nachrichtlich_Abgaenge_in_2024_Anzahl  \n",
      "0                               225380.0  \n",
      "1                               138090.0  \n",
      "2                                87290.0  \n",
      "3                                 7140.0  \n",
      "4                                 4370.0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import and test  Excel loading functionality\n",
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/statistischer-bericht-auslaend-bevoelkerung-2010200247005.xlsx\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb13b5",
   "metadata": {},
   "source": [
    "# JSON Loading TEST SmartAutoDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f07b2e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: icecreamshop.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'list'>\n",
      "   List length: 2028\n",
      "   First item keys: ['event_timestamp', 'user_id', 'item', 'price', 'quantity']\n",
      "   ğŸ“Š Standard flattening: (2028, 5)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 2028, 'max_nesting_depth': 1, 'nested_arrays': 1, 'nested_objects': 3, 'total_potential_columns': 15, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (2028, 5)\n",
      "   ğŸ“ˆ Improvement: 1.0x more columns\n",
      "   âœ… Using standard flattening (minimal benefit from deep)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 2028 rows, 5 columns\n",
      "   ğŸ“Š Columns: ['event_timestamp', 'user_id', 'item', 'price', 'quantity']\n",
      "     event_timestamp   user_id               item  price  quantity\n",
      "0  8/19/2025 0:02:31  User_265  Strawberry Sorbet     13         4\n",
      "1  8/19/2025 0:00:32   User_44  Vanilla Ice Cream     12         1\n",
      "2  8/19/2025 0:00:32  User_254   Chocolate Sundae     11         3\n",
      "3  8/19/2025 0:02:33  User_227  Strawberry Sorbet      2         3\n",
      "4  8/19/2025 0:00:33  User_948   Chocolate Sundae      9         4\n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/icecreamshop.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bf68855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: old_rest_data.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'list'>\n",
      "   List length: 9651\n",
      "   First item keys: ['id', 'object_name', 'address', 'chain', 'object_type', 'number']\n",
      "   ğŸ“Š Standard flattening: (9651, 6)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 9651, 'max_nesting_depth': 1, 'nested_arrays': 1, 'nested_objects': 3, 'total_potential_columns': 18, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (9651, 6)\n",
      "   ğŸ“ˆ Improvement: 1.0x more columns\n",
      "   âœ… Using standard flattening (minimal benefit from deep)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 9651 rows, 6 columns\n",
      "   ğŸ“Š Columns: ['id', 'object_name', 'address', 'chain', 'object_type', 'number']\n",
      "      id          object_name                    address  chain object_type  \\\n",
      "0  11786  HABITAT COFFEE SHOP     3708 N EAGLE ROCK BLVD  FALSE        Cafe   \n",
      "1  11787             REILLY'S        100 WORLD WAY # 120  FALSE  Restaurant   \n",
      "2  11788       STREET CHURROS  6801 HOLLYWOOD BLVD # 253  FALSE   Fast Food   \n",
      "3  11789    TRINITI ECHO PARK         1814 W SUNSET BLVD  FALSE  Restaurant   \n",
      "4  11790               POLLEN         2100 ECHO PARK AVE  FALSE  Restaurant   \n",
      "\n",
      "   number  \n",
      "0      26  \n",
      "1       9  \n",
      "2      20  \n",
      "3      22  \n",
      "4      20  \n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/old_rest_data.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f22bd8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: orders_heavy.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'list'>\n",
      "   List length: 150281\n",
      "   First item keys: ['', 'order_date', 'sales_qty', 'sales_amount', 'currency', 'user_id', 'r_id']\n",
      "   ğŸ“Š Standard flattening: (150281, 7)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 150281, 'max_nesting_depth': 1, 'nested_arrays': 1, 'nested_objects': 3, 'total_potential_columns': 21, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (150281, 7)\n",
      "   ğŸ“ˆ Improvement: 1.0x more columns\n",
      "   âœ… Using standard flattening (minimal benefit from deep)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   âœ… Found date column: 'order_date' (%Y-%m-%d)\n",
      "   ğŸ“… Total date columns found: 1\n",
      "âœ… JSON loaded: 150281 rows, 7 columns\n",
      "   ğŸ“Š Columns: ['', 'order_date', 'sales_qty', 'sales_amount', 'currency', 'user_id', 'r_id']\n",
      "     order_date  sales_qty  sales_amount currency  user_id    r_id\n",
      "0  0 2017-10-10        100         41241      INR    49226  567335\n",
      "1  1 2018-05-08          3            -1      INR    77359  531342\n",
      "2  2 2018-04-06          1           875      INR     5321  158203\n",
      "3  3 2018-04-11          1           583      INR    21343  187912\n",
      "4  4 2018-06-18          6          7176      INR    75378  543530\n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/orders_heavy.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f65545e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: vivino_data.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'list'>\n",
      "   List length: 7500\n",
      "   First item keys: ['winery', 'wine', 'year', 'rating', 'num_reviews', 'country', 'region', 'price', 'type', 'body', 'acidity']\n",
      "   ğŸ“Š Standard flattening: (7500, 11)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 7500, 'max_nesting_depth': 1, 'nested_arrays': 1, 'nested_objects': 3, 'total_potential_columns': 33, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (7500, 11)\n",
      "   ğŸ“ˆ Improvement: 1.0x more columns\n",
      "   âœ… Using standard flattening (minimal benefit from deep)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 7500 rows, 11 columns\n",
      "   ğŸ“Š Columns: ['winery', 'wine', 'year', 'rating', 'num_reviews', 'country', 'region', 'price', 'type', 'body', 'acidity']\n",
      "          winery           wine  year  rating  num_reviews country  \\\n",
      "0  Teso La Monja          Tinto  2013     4.9           58  Espana   \n",
      "1         Artadi  Vina El Pison  2018     4.9           31  Espana   \n",
      "2   Vega Sicilia          Unico  2009     4.8         1793  Espana   \n",
      "3   Vega Sicilia          Unico  1999     4.8         1705  Espana   \n",
      "4   Vega Sicilia          Unico  1996     4.8         1309  Espana   \n",
      "\n",
      "             region   price                  type body acidity  \n",
      "0              Toro  995.00              Toro Red    5       3  \n",
      "1    Vino de Espana  313.50           Tempranillo    4       2  \n",
      "2  Ribera del Duero  324.95  Ribera Del Duero Red    5       3  \n",
      "3  Ribera del Duero  692.96  Ribera Del Duero Red    5       3  \n",
      "4  Ribera del Duero  778.06  Ribera Del Duero Red    5       3  \n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/vivino_data.json\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87cacfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: vivino_data.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'list'>\n",
      "   List length: 7500\n",
      "   First item keys: ['winery', 'wine', 'year', 'rating', 'num_reviews', 'country', 'region', 'price', 'type', 'body', 'acidity']\n",
      "   ğŸ“Š Standard flattening: (7500, 11)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 7500, 'max_nesting_depth': 1, 'nested_arrays': 1, 'nested_objects': 3, 'total_potential_columns': 33, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (7500, 11)\n",
      "   ğŸ“ˆ Improvement: 1.0x more columns\n",
      "   âœ… Using standard flattening (minimal benefit from deep)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 7500 rows, 11 columns\n",
      "   ğŸ“Š Columns: ['winery', 'wine', 'year', 'rating', 'num_reviews', 'country', 'region', 'price', 'type', 'body', 'acidity']\n",
      "          winery           wine  year  rating  num_reviews country  \\\n",
      "0  Teso La Monja          Tinto  2013     4.9           58  Espana   \n",
      "1         Artadi  Vina El Pison  2018     4.9           31  Espana   \n",
      "2   Vega Sicilia          Unico  2009     4.8         1793  Espana   \n",
      "3   Vega Sicilia          Unico  1999     4.8         1705  Espana   \n",
      "4   Vega Sicilia          Unico  1996     4.8         1309  Espana   \n",
      "\n",
      "             region   price                  type body acidity  \n",
      "0              Toro  995.00              Toro Red    5       3  \n",
      "1    Vino de Espana  313.50           Tempranillo    4       2  \n",
      "2  Ribera del Duero  324.95  Ribera Del Duero Red    5       3  \n",
      "3  Ribera del Duero  692.96  Ribera Del Duero Red    5       3  \n",
      "4  Ribera del Duero  778.06  Ribera Del Duero Red    5       3  \n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/vivino_data.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95482e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ebe60e8",
   "metadata": {},
   "source": [
    "# PROBLEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4561554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: s1_line_detail.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'dict'>\n",
      "   Keys: ['type', 'id', 'name', 'operator', 'mode', 'product', 'variants']\n",
      "   ğŸ“Š Standard flattening: (1, 7)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 108, 'max_nesting_depth': 4, 'nested_arrays': 2, 'nested_objects': 4, 'total_potential_columns': 9, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (1, 95)\n",
      "   ğŸ“ˆ Improvement: 13.6x more columns\n",
      "   âœ… Using deep flattening (better data extraction)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 1 rows, 95 columns\n",
      "   ğŸ“Š First 10 columns: ['type', 'id', 'name', 'operator', 'mode', 'product', 'variants_0_stops', 'variants_0_trips', 'variants_1_stops', 'variants_1_trips']... (+85 more)\n",
      "   type         id name operator   mode product  \\\n",
      "0  line  17442_900  M13      796  train    tram   \n",
      "\n",
      "                                    variants_0_stops  variants_0_trips  \\\n",
      "0  de:11000:900160521::5, de:11000:900160012::1, ...                16   \n",
      "\n",
      "                                    variants_1_stops  variants_1_trips  ...  \\\n",
      "0  de:11000:900120021::6, de:11000:900120527::1, ...                 6  ...   \n",
      "\n",
      "     variants_21_stops_5    variants_21_stops_6     variants_21_stops_7  \\\n",
      "0  de:11000:900140011::7  de:11000:900140005::6  de:11000:900140006::10   \n",
      "\n",
      "     variants_21_stops_8    variants_21_stops_9  variants_21_trips  \\\n",
      "0  de:11000:900140012::4  de:11000:900140713::4                  6   \n",
      "\n",
      "                                   variants_22_stops  variants_22_trips  \\\n",
      "0  de:11000:900141506::6, de:11000:900141507::4, ...                 35   \n",
      "\n",
      "                                   variants_23_stops  variants_23_trips  \n",
      "0  de:11000:900140017::8, de:11000:900140505::2, ...                  1  \n",
      "\n",
      "[1 rows x 95 columns]\n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/s1_line_detail.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d8c77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ” JSON FLATTENING DIAGNOSIS ===\n",
      "Raw data type: <class 'dict'>\n",
      "Source data: Single dictionary object\n",
      "workflowSteps: 0 elements\n",
      "pandas.json_normalize: (1, 7)\n",
      "Deep flattening: (1, 54)\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSIS: Flatten columns?\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/s1_line_detail.json\"\n",
    "\n",
    "print(\"=== ğŸ” JSON FLATTENING DIAGNOSIS ===\")\n",
    "\n",
    "# Load raw data\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Raw data type: {type(raw_data)}\")\n",
    "\n",
    "# Handle different JSON structures\n",
    "if isinstance(raw_data, list):\n",
    "    print(f\"Source data: {len(raw_data)} records\")\n",
    "    if len(raw_data) > 0:\n",
    "        first_item = raw_data[0]\n",
    "        workflow_steps = first_item.get('workflowSteps', [])\n",
    "        print(f\"workflowSteps: {len(workflow_steps)} elements\")\n",
    "        \n",
    "        if workflow_steps:\n",
    "            input_rows = workflow_steps[0].get('inputRows', [])\n",
    "            print(f\"inputRows in first step: {len(input_rows)} elements\")\n",
    "    else:\n",
    "        print(\"Empty list - no records to analyze\")\n",
    "        first_item = {}\n",
    "elif isinstance(raw_data, dict):\n",
    "    print(\"Source data: Single dictionary object\")\n",
    "    first_item = raw_data\n",
    "    workflow_steps = first_item.get('workflowSteps', [])\n",
    "    print(f\"workflowSteps: {len(workflow_steps)} elements\")\n",
    "    \n",
    "    if workflow_steps:\n",
    "        input_rows = workflow_steps[0].get('inputRows', [])\n",
    "        print(f\"inputRows in first step: {len(input_rows)} elements\")\n",
    "    \n",
    "    # Convert to list for processing\n",
    "    raw_data = [raw_data]\n",
    "else:\n",
    "    print(f\"Unexpected data type: {type(raw_data)}\")\n",
    "    raw_data = []\n",
    "\n",
    "# Test pandas normalize only if we have data\n",
    "if raw_data:\n",
    "    try:\n",
    "        df_pandas = pd.json_normalize(raw_data, sep='_', max_level=None)\n",
    "        print(f\"pandas.json_normalize: {df_pandas.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"pandas.json_normalize failed: {e}\")\n",
    "\n",
    "    # Deep flattening\n",
    "    def deep_flatten(data, sep='_', prefix=''):\n",
    "        result = {}\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                new_key = f\"{prefix}{sep}{key}\" if prefix else key\n",
    "                if isinstance(value, dict):\n",
    "                    result.update(deep_flatten(value, sep, new_key))\n",
    "                elif isinstance(value, list):\n",
    "                    if value and isinstance(value[0], dict):\n",
    "                        for i, item in enumerate(value):\n",
    "                            result.update(deep_flatten(item, sep, f\"{new_key}_{i}\"))\n",
    "                    else:\n",
    "                        result[new_key] = str(value) if value else ''\n",
    "                else:\n",
    "                    result[new_key] = value\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        flattened_records = [deep_flatten(item) for item in raw_data]\n",
    "        df_deep = pd.DataFrame(flattened_records)\n",
    "        print(f\"Deep flattening: {df_deep.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Deep flattening failed: {e}\")\n",
    "else:\n",
    "    print(\"No data to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07822b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: socrata_metadata.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'dict'>\n",
      "   Keys: ['id', 'name', 'assetType', 'attribution', 'attributionLink', 'averageRating', 'category', 'createdAt', 'description', 'displayType', 'downloadCount', 'hideFromCatalog', 'hideFromDataJson', 'indexUpdatedAt', 'newBackend', 'numberOfComments', 'oid', 'provenance', 'publicationAppendEnabled', 'publicationDate', 'publicationGroup', 'publicationStage', 'rowClass', 'rowsUpdatedAt', 'rowsUpdatedBy', 'tableId', 'totalTimesRated', 'viewCount', 'viewLastModified', 'viewType', 'approvals', 'columns', 'grants', 'metadata', 'owner', 'query', 'rights', 'tableAuthor', 'tags', 'flags']\n",
      "   ğŸ“Š Standard flattening: (1, 64)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 101, 'max_nesting_depth': 6, 'nested_arrays': 10, 'nested_objects': 39, 'total_potential_columns': 132, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (1, 1773)\n",
      "   ğŸ“ˆ Improvement: 27.7x more columns\n",
      "   âœ… Using deep flattening (better data extraction)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 1 rows, 1773 columns\n",
      "   ğŸ“Š First 10 columns: ['id', 'name', 'assetType', 'attribution', 'attributionLink', 'averageRating', 'category', 'createdAt', 'description', 'displayType']... (+1763 more)\n",
      "          id                              name assetType  \\\n",
      "0  qybk-bjjc  2010 - 2016 School Safety Report   dataset   \n",
      "\n",
      "                     attribution  \\\n",
      "0  Department of Education (DOE)   \n",
      "\n",
      "                                     attributionLink  averageRating  \\\n",
      "0  http://schools.nyc.gov/OurSchools/SchoolSafety...              0   \n",
      "\n",
      "    category   createdAt                                        description  \\\n",
      "0  Education  1361423795  Since 1998, the New York City Police Departmen...   \n",
      "\n",
      "  displayType  ...  tableAuthor_flags_1  tags_0  tags_1  tags_2  tags_3  \\\n",
      "0       table  ...  mayBeStoriesCoOwner  school  safety  report    nypd   \n",
      "\n",
      "              tags_4  flags_0              flags_1     flags_2  \\\n",
      "0  lifelong learning  default  ownerMayBeContacted  restorable   \n",
      "\n",
      "                  flags_3  \n",
      "0  restorePossibleForType  \n",
      "\n",
      "[1 rows x 1773 columns]\n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/socrata_metadata.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: tasks.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'dict'>\n",
      "   Keys: ['data']\n",
      "   ğŸ“Š Standard flattening: (1, 1)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 42, 'max_nesting_depth': 6, 'nested_arrays': 11, 'nested_objects': 22, 'total_potential_columns': 111, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (1, 1184)\n",
      "   ğŸ“ˆ Improvement: 1184.0x more columns\n",
      "   âœ… Using deep flattening (better data extraction)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   âœ… Found date column: 'data_0_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_0_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_1_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_1_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_2_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_2_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_3_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_3_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_4_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_4_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_5_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_5_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_6_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_6_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_7_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_7_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_7_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_8_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_8_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_8_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_9_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_9_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_10_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_10_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_11_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_11_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_12_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_12_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_12_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_13_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_13_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_13_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_14_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_14_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_14_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_15_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_15_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_15_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_16_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_16_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_16_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_17_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_17_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_18_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_18_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_19_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_19_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_20_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_20_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_21_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_21_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_21_due_on' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_21_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_22_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_22_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_22_due_on' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_22_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_23_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_23_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_23_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_24_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_24_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_24_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_25_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_25_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_25_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_26_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_26_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_27_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_27_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_27_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_28_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_28_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_28_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_29_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_29_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_29_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_30_completed_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_30_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_30_modified_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_31_created_at' (%Y-%m-%d)\n",
      "   âœ… Found date column: 'data_31_modified_at' (%Y-%m-%d)\n",
      "   ğŸ“… Total date columns found: 82\n",
      "âœ… JSON loaded: 1 rows, 1184 columns\n",
      "   ğŸ“Š First 10 columns: ['data_0_gid', 'data_0_actual_time_minutes', 'data_0_assignee', 'data_0_assignee_status', 'data_0_completed', 'data_0_completed_at', 'data_0_created_at', 'data_0_due_at', 'data_0_due_on', 'data_0_followers_0_gid']... (+1174 more)\n",
      "         data_0_gid data_0_actual_time_minutes data_0_assignee  \\\n",
      "0  1209741953941921                       None            None   \n",
      "\n",
      "  data_0_assignee_status  data_0_completed data_0_completed_at  \\\n",
      "0               upcoming             False                None   \n",
      "\n",
      "  data_0_created_at data_0_due_at data_0_due_on data_0_followers_0_gid  ...  \\\n",
      "0               NaT          None          None       1204096118745029  ...   \n",
      "\n",
      "  data_31_projects_0_gid data_31_projects_0_name  \\\n",
      "0       1209206925261347                  Webeet   \n",
      "\n",
      "   data_31_projects_0_resource_type  data_31_resource_type data_31_start_at  \\\n",
      "0                           project                   task             None   \n",
      "\n",
      "  data_31_start_on data_31_resource_subtype data_31_workspace_gid  \\\n",
      "0             None             default_task      1204095851505012   \n",
      "\n",
      "  data_31_workspace_name data_31_workspace_resource_type  \n",
      "0           My Workspace                       workspace  \n",
      "\n",
      "[1 rows x 1184 columns]\n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/tasks.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0dc8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SmartAutoDataLoader ready!\n",
      "ğŸ¯ Loading file: s1_line_detail.json\n",
      "ğŸ” Format detected: json\n",
      "ğŸ—‚ï¸ Loading JSON file with intelligent analysis...\n",
      "   ğŸ“‹ JSON structure analysis:\n",
      "   Type: <class 'dict'>\n",
      "   Keys: ['type', 'id', 'name', 'operator', 'mode', 'product', 'variants']\n",
      "   ğŸ“Š Standard flattening: (1, 7)\n",
      "   ğŸ§  Complexity analysis: {'total_items': 108, 'max_nesting_depth': 4, 'nested_arrays': 2, 'nested_objects': 4, 'total_potential_columns': 9, 'needs_deep_flattening': True}\n",
      "   ğŸ” Applying deep flattening...\n",
      "   ğŸ“Š Deep flattening: (1, 95)\n",
      "   ğŸ“ˆ Improvement: 13.6x more columns\n",
      "   âœ… Using deep flattening (better data extraction)\n",
      "ğŸ—“ï¸ Searching for date columns...\n",
      "   ğŸ“… No date columns detected\n",
      "âœ… JSON loaded: 1 rows, 95 columns\n",
      "   ğŸ“Š First 10 columns: ['type', 'id', 'name', 'operator', 'mode', 'product', 'variants_0_stops', 'variants_0_trips', 'variants_1_stops', 'variants_1_trips']... (+85 more)\n",
      "   type         id name operator   mode product  \\\n",
      "0  line  17442_900  M13      796  train    tram   \n",
      "\n",
      "                                    variants_0_stops  variants_0_trips  \\\n",
      "0  de:11000:900160521::5, de:11000:900160012::1, ...                16   \n",
      "\n",
      "                                    variants_1_stops  variants_1_trips  ...  \\\n",
      "0  de:11000:900120021::6, de:11000:900120527::1, ...                 6  ...   \n",
      "\n",
      "     variants_21_stops_5    variants_21_stops_6     variants_21_stops_7  \\\n",
      "0  de:11000:900140011::7  de:11000:900140005::6  de:11000:900140006::10   \n",
      "\n",
      "     variants_21_stops_8    variants_21_stops_9  variants_21_trips  \\\n",
      "0  de:11000:900140012::4  de:11000:900140713::4                  6   \n",
      "\n",
      "                                   variants_22_stops  variants_22_trips  \\\n",
      "0  de:11000:900141506::6, de:11000:900141507::4, ...                 35   \n",
      "\n",
      "                                   variants_23_stops  variants_23_trips  \n",
      "0  de:11000:900140017::8, de:11000:900140505::2, ...                  1  \n",
      "\n",
      "[1 rows x 95 columns]\n"
     ]
    }
   ],
   "source": [
    "from smart_auto_data_loader import SmartAutoDataLoader\n",
    "\n",
    "loader = SmartAutoDataLoader()\n",
    "df = loader.load(\"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/s1_line_detail.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a739773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ” JSON FLATTENING DIAGNOSIS ===\n",
      "Raw data type: <class 'dict'>\n",
      "Source data: Single dictionary object\n",
      "workflowSteps: 0 elements\n",
      "pandas.json_normalize: (1, 7)\n",
      "Deep flattening: (1, 54)\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSIS: Flatten columns?\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/s1_line_detail.json\"\n",
    "\n",
    "print(\"=== ğŸ” JSON FLATTENING DIAGNOSIS ===\")\n",
    "\n",
    "# Load raw data\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Raw data type: {type(raw_data)}\")\n",
    "\n",
    "# Handle different JSON structures\n",
    "if isinstance(raw_data, list):\n",
    "    print(f\"Source data: {len(raw_data)} records\")\n",
    "    if len(raw_data) > 0:\n",
    "        first_item = raw_data[0]\n",
    "        workflow_steps = first_item.get('workflowSteps', [])\n",
    "        print(f\"workflowSteps: {len(workflow_steps)} elements\")\n",
    "        \n",
    "        if workflow_steps:\n",
    "            input_rows = workflow_steps[0].get('inputRows', [])\n",
    "            print(f\"inputRows in first step: {len(input_rows)} elements\")\n",
    "    else:\n",
    "        print(\"Empty list - no records to analyze\")\n",
    "        first_item = {}\n",
    "elif isinstance(raw_data, dict):\n",
    "    print(\"Source data: Single dictionary object\")\n",
    "    first_item = raw_data\n",
    "    workflow_steps = first_item.get('workflowSteps', [])\n",
    "    print(f\"workflowSteps: {len(workflow_steps)} elements\")\n",
    "    \n",
    "    if workflow_steps:\n",
    "        input_rows = workflow_steps[0].get('inputRows', [])\n",
    "        print(f\"inputRows in first step: {len(input_rows)} elements\")\n",
    "    \n",
    "    # Convert to list for processing\n",
    "    raw_data = [raw_data]\n",
    "else:\n",
    "    print(f\"Unexpected data type: {type(raw_data)}\")\n",
    "    raw_data = []\n",
    "\n",
    "# Test pandas normalize only if we have data\n",
    "if raw_data:\n",
    "    try:\n",
    "        df_pandas = pd.json_normalize(raw_data, sep='_', max_level=None)\n",
    "        print(f\"pandas.json_normalize: {df_pandas.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"pandas.json_normalize failed: {e}\")\n",
    "\n",
    "    # Deep flattening\n",
    "    def deep_flatten(data, sep='_', prefix=''):\n",
    "        result = {}\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                new_key = f\"{prefix}{sep}{key}\" if prefix else key\n",
    "                if isinstance(value, dict):\n",
    "                    result.update(deep_flatten(value, sep, new_key))\n",
    "                elif isinstance(value, list):\n",
    "                    if value and isinstance(value[0], dict):\n",
    "                        for i, item in enumerate(value):\n",
    "                            result.update(deep_flatten(item, sep, f\"{new_key}_{i}\"))\n",
    "                    else:\n",
    "                        result[new_key] = str(value) if value else ''\n",
    "                else:\n",
    "                    result[new_key] = value\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        flattened_records = [deep_flatten(item) for item in raw_data]\n",
    "        df_deep = pd.DataFrame(flattened_records)\n",
    "        print(f\"Deep flattening: {df_deep.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Deep flattening failed: {e}\")\n",
    "else:\n",
    "    print(\"No data to process\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
