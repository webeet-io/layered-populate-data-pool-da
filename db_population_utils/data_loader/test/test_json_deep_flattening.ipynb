{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cabed4",
   "metadata": {},
   "source": [
    "# Test Deep JSON Flattening for KG_Export_Aufträge.json\n",
    "\n",
    "This notebook tests proper deep flattening of the complex JSON structure to extract all nested workflow and input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add data_loader to path\n",
    "sys.path.append('.')\n",
    "\n",
    "print(\"🔍 JSON Deep Flattening Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# File path\n",
    "json_file = \"/Users/svitlanakovalivska/layered-populate-data-pool-da/db_population_utils/data/KG_Export_Aufträge.json\"\n",
    "\n",
    "# Check if file exists\n",
    "if Path(json_file).exists():\n",
    "    print(f\"✅ File found: {Path(json_file).name}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze JSON structure\n",
    "print(\"=== 📋 JSON STRUCTURE ANALYSIS ===\")\n",
    "\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"📊 Root structure: {type(raw_data)}\")\n",
    "print(f\"📊 Number of records: {len(raw_data)}\")\n",
    "\n",
    "# Analyze first record\n",
    "first_item = raw_data[0]\n",
    "print(f\"📊 Top-level keys ({len(first_item)}): {list(first_item.keys())}\")\n",
    "\n",
    "# Analyze workflowSteps\n",
    "workflow_steps = first_item.get('workflowSteps', [])\n",
    "print(f\"\\n🔸 workflowSteps: {len(workflow_steps)} elements\")\n",
    "\n",
    "if workflow_steps:\n",
    "    first_step = workflow_steps[0]\n",
    "    step_keys = list(first_step.keys())\n",
    "    print(f\"🔸 First step keys ({len(step_keys)}): {step_keys}\")\n",
    "    \n",
    "    # Analyze inputRows\n",
    "    input_rows = first_step.get('inputRows', [])\n",
    "    print(f\"🔸 inputRows in first step: {len(input_rows)} elements\")\n",
    "    \n",
    "    if input_rows:\n",
    "        first_input = input_rows[0]\n",
    "        input_keys = list(first_input.keys())\n",
    "        print(f\"🔸 First input keys ({len(input_keys)}): {input_keys}\")\n",
    "        \n",
    "        # Check for dropdown options\n",
    "        dropdown_options = first_input.get('dropdownOptions', [])\n",
    "        if dropdown_options:\n",
    "            print(f\"🔸 Dropdown options: {len(dropdown_options)} items\")\n",
    "            if isinstance(dropdown_options[0], dict):\n",
    "                print(f\"🔸 Dropdown structure: {list(dropdown_options[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec08462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different flattening approaches\n",
    "print(\"\\n=== 🧪 FLATTENING COMPARISON ===\")\n",
    "\n",
    "# 1. Standard pandas normalize\n",
    "print(\"1️⃣ Testing pandas.json_normalize...\")\n",
    "df_pandas = pd.json_normalize(raw_data, sep='_', max_level=None)\n",
    "print(f\"   Result: {df_pandas.shape}\")\n",
    "\n",
    "# 2. Deep flattening function\n",
    "print(\"2️⃣ Testing deep flattening...\")\n",
    "\n",
    "def deep_flatten(data, sep='_', prefix=''):\n",
    "    \"\"\"Recursively flatten all nested structures\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_key = f\"{prefix}{sep}{key}\" if prefix else key\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                # Recursively flatten nested dictionaries\n",
    "                result.update(deep_flatten(value, sep, new_key))\n",
    "            elif isinstance(value, list):\n",
    "                if value and isinstance(value[0], dict):\n",
    "                    # List of dictionaries - flatten each item\n",
    "                    for i, item in enumerate(value):\n",
    "                        item_key = f\"{new_key}_{i}\"\n",
    "                        result.update(deep_flatten(item, sep, item_key))\n",
    "                else:\n",
    "                    # Simple list - convert to string\n",
    "                    result[new_key] = ', '.join(str(x) for x in value) if value else ''\n",
    "            else:\n",
    "                # Simple value\n",
    "                result[new_key] = value\n",
    "    else:\n",
    "        result[prefix] = data\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply deep flattening\n",
    "flattened_records = []\n",
    "for item in raw_data:\n",
    "    flattened_item = deep_flatten(item)\n",
    "    flattened_records.append(flattened_item)\n",
    "\n",
    "df_deep = pd.DataFrame(flattened_records)\n",
    "print(f\"   Result: {df_deep.shape}\")\n",
    "\n",
    "print(f\"\\n📊 COMPARISON:\")\n",
    "print(f\"   pandas.json_normalize: {df_pandas.shape[1]} columns\")\n",
    "print(f\"   Deep flattening:       {df_deep.shape[1]} columns\")\n",
    "print(f\"   Improvement:           {df_deep.shape[1] - df_pandas.shape[1]} additional columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4861cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the deep flattening results\n",
    "print(\"=== 📊 DEEP FLATTENING ANALYSIS ===\")\n",
    "\n",
    "print(f\"✅ Total columns extracted: {len(df_deep.columns)}\")\n",
    "\n",
    "# Categorize columns\n",
    "base_cols = [col for col in df_deep.columns if '_' not in col]\n",
    "workflow_cols = [col for col in df_deep.columns if 'workflow' in col.lower()]\n",
    "input_cols = [col for col in df_deep.columns if 'input' in col.lower()]\n",
    "dropdown_cols = [col for col in df_deep.columns if 'dropdown' in col.lower()]\n",
    "other_nested = [col for col in df_deep.columns if col not in base_cols + workflow_cols + input_cols + dropdown_cols]\n",
    "\n",
    "print(f\"\\n📋 Column Categories:\")\n",
    "print(f\"   🔸 Base fields:        {len(base_cols)} columns\")\n",
    "print(f\"   🔸 Workflow fields:    {len(workflow_cols)} columns\") \n",
    "print(f\"   🔸 Input fields:       {len(input_cols)} columns\")\n",
    "print(f\"   🔸 Dropdown fields:    {len(dropdown_cols)} columns\")\n",
    "print(f\"   🔸 Other nested:       {len(other_nested)} columns\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n📝 Examples:\")\n",
    "if base_cols:\n",
    "    print(f\"   Base: {base_cols[:5]}\")\n",
    "if workflow_cols:\n",
    "    print(f\"   Workflow: {workflow_cols[:3]}\")\n",
    "if input_cols:\n",
    "    print(f\"   Input: {input_cols[:3]}\")\n",
    "if dropdown_cols:\n",
    "    print(f\"   Dropdown: {dropdown_cols[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d27243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data from key columns\n",
    "print(\"=== 📄 SAMPLE DATA ===\")\n",
    "\n",
    "# Base information\n",
    "print(\"🔸 Base Information (first record):\")\n",
    "for col in base_cols[:5]:\n",
    "    value = df_deep[col].iloc[0]\n",
    "    print(f\"   {col}: {value}\")\n",
    "\n",
    "# Workflow information\n",
    "if workflow_cols:\n",
    "    print(f\"\\n🔸 Workflow Information (first few columns):\")\n",
    "    for col in workflow_cols[:5]:\n",
    "        value = df_deep[col].iloc[0]\n",
    "        print(f\"   {col}: {value}\")\n",
    "\n",
    "# Input information  \n",
    "if input_cols:\n",
    "    print(f\"\\n🔸 Input Information (first few columns):\")\n",
    "    for col in input_cols[:5]:\n",
    "        value = df_deep[col].iloc[0]\n",
    "        print(f\"   {col}: {value}\")\n",
    "\n",
    "# Data quality check\n",
    "print(f\"\\n🔍 Data Quality:\")\n",
    "print(f\"   Total rows: {len(df_deep)}\")\n",
    "print(f\"   Non-null values per column (first 10):\")\n",
    "    \n",
    "for col in df_deep.columns[:10]:\n",
    "    non_null_count = df_deep[col].notna().sum()\n",
    "    print(f\"   {col}: {non_null_count}/{len(df_deep)} ({non_null_count/len(df_deep)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ed187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results and summary\n",
    "print(\"=== 💾 EXPORT AND SUMMARY ===\")\n",
    "\n",
    "# Save the flattened data\n",
    "output_file = \"flattened_json_data.csv\"\n",
    "df_deep.to_csv(output_file, index=False)\n",
    "print(f\"✅ Flattened data saved to: {output_file}\")\n",
    "\n",
    "# Create summary\n",
    "print(f\"\\n📊 FINAL SUMMARY:\")\n",
    "print(f\"   📁 Source file: {Path(json_file).name}\")\n",
    "print(f\"   📊 Source records: {len(raw_data)}\")\n",
    "print(f\"   📊 Standard flattening: {df_pandas.shape[1]} columns\")\n",
    "print(f\"   📊 Deep flattening: {df_deep.shape[1]} columns\")\n",
    "print(f\"   📊 Data extraction improvement: {df_deep.shape[1]/df_pandas.shape[1]:.1f}x more data\")\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSION:\")\n",
    "print(f\"   ✅ Deep flattening successfully extracts {df_deep.shape[1]} columns\")\n",
    "print(f\"   ✅ This includes all workflow steps and input data\")\n",
    "print(f\"   ✅ This approach should be implemented in SmartAutoDataLoader\")\n",
    "\n",
    "# Show memory usage\n",
    "memory_mb = df_deep.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\n💾 Memory usage: {memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1b048",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "The deep flattening approach successfully extracts all nested data from the complex JSON structure:\n",
    "\n",
    "- **Standard pandas.json_normalize**: 22 columns\n",
    "- **Deep flattening**: 504+ columns  \n",
    "- **Improvement**: 23x more data extracted\n",
    "\n",
    "This demonstrates that the SmartAutoDataLoader needs to be updated to use deep flattening instead of the standard pandas approach for JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deep.head()  # Display the first few rows of the deep flattened DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
