# â€œTransforming, enriching, and populating Berlin supermarket data with district-level metadata using spatial joins and PostgreSQL.â€

# Step 1: ğŸ›’ Berlin Superstore Dataset â€“ Data Profiling & Preparation**

This project involves extracting, exploring, and transforming supermarket data in Berlin using OpenStreetMap (OSM) via the osmnx library. The processed dataset can be used for location-based services, business mapping, or urban retail studies.

â¸»

## ğŸ“Œ 1. Data Source Discovery

	â€¢	Source: OpenStreetMap (OSM)
	â€¢	Accessed via: osmnx Python package
	â€¢	Query Tags: {"shop": "supermarket"}
	â€¢	Place Queried: "Berlin, Germany"


Topic: Supermarkets in Berlin

**Main source:**

- Name: OpenStreetMap (OSM) via OSMnx library
- Source and origin: Public crowdsourced geospatial database
- Update frequency: Continuous (dynamic)
- Data type: Dynamic (API query using shop=supermarket)

**Reason for selection:**

- Covers all supermarkets in Berlin
- Includes coordinates, names, addresses, and other useful attributes
- Open, free, and easy to query programmatically

**Optional additional sources:**

- Name: Berlin Open Data Portal (daten.berlin.de)
- Source and origin: Official Berlin city government
- Update frequency: Varies per dataset
- Data type: Static or semi-static (download as CSV/GeoJSON)
- Possible usage: Enrich with official administrative boundaries or extra metadata

**Enrichment potential:**

Neighborhood/district info from Berlin shapefiles (GeoJSON)
Linking to local amenities for spatial context

â¸»

## ğŸ”„ 2. Planned Transformation & Profiling Steps

**âœ… Step-by-Step Process**

	1.	ğŸ—ºï¸ Data Extraction

	â€¢	Queried Berlin OSM boundaries using osmnx.features_from_place(...)
	â€¢	Filtered features using shop=supermarket tag

	2.	ğŸ§¹ Initial Cleaning & Column Selection

	â€¢	Extracted relevant columns:
name, addr:street, addr:housenumber, addr:postcode, addr:city, opening_hours, brand, geometry, and various payment:* tags
	â€¢	Extracted latitude and longitude from geometry
	â€¢	Renamed columns for clarity (e.g., addr:street â†’ street, brand â†’ store_type, etc.)

	3.	ğŸ“‹ Dataset Profiling

	â€¢	.shape â€“ Number of rows and columns
	â€¢	.info() â€“ Data types and null counts
	â€¢	Missing value analysis â€“ Count + percentage of missing values per column
	â€¢	Distinct values per column â€“ For understanding cardinality
	â€¢	Most common values â€“ Using value_counts() for categorical profiling

	4.	ğŸ“ Spatial Sanity Checks

	â€¢	Verified geometry types (Point vs MultiPoint)
	â€¢	Confirmed latitude and longitude ranges fall within Berlinâ€™s bounding box

	5.	ğŸ’¾ Data Export

	â€¢	Raw dataset saved in sources folder as:
	â€¢	CSV: supermarkets_raw.csv without geometry
	â€¢	GeoJSON: supermarkets_raw.geojson with geometry (for mapping use)

â¸»

## ğŸ§° Tools Used**

	â€¢	Python
	â€¢	osmnx, geopandas, pandas
	â€¢	Jupyter Notebook (for analysis and exploration)

# Step 2: ğŸª Berlin Superstores â€“ Data Transformation & DB Insert

This project documents the transformation and integration process for a dataset of supermarkets (and similar stores) in Berlin. The data was sourced from **OpenStreetMap (OSM)** using the **Overpass API** and further enriched with geographic and administrative context before being inserted into a **Neon test database**.

---

## ğŸ—‚ï¸ 1. Data Source

- âœ… **Source**: OpenStreetMap (via `osmnx` and Overpass API)
- âœ… **Tags used**: `shop=supermarket`, `convenience`, `grocery`, `department_store`, `general`
- âœ… **Format**: JSON â†’ GeoDataFrame â†’ Cleaned DataFrame

---

## ğŸ”„ 2. Data Transformation Steps

All transformation steps were handled in Python using `pandas` and `geopandas`.

### âœ… Cleaning & Normalization
- Cleaned column names (lowercase, snake_case, removed special characters)
- Standardized string fields (`name`, `brand`, `address`, etc.)
- Converted `NaN` to `None` where needed for DB compatibility

### âœ… Enrichment
- Extracted `latitude` and `longitude` from geometry
- Reverse geocoded missing address fields (`street`, `housenumber`, `postcode`, `city`)
- Joined **district** and **neighborhood** info using Berlin LOR GeoJSON boundaries
- Added new fields: `source`, `store_id`, `full_address`

### âœ… Data Validation
- Checked data types and missing values
- Ensured geometry points are valid and matched with admin boundaries
- Normalized contact fields (`phone`, `email`, `website`)
- Dropped irrelevant or overly sparse columns

---

## ğŸ—ƒï¸ 3. Planned Database Schema (`superstore_in_berlin`)

| Column              | Type      | Description                                      |
|---------------------|-----------|-----------------------------------------------   |
| `store_id`          | string    | Unique ID from OSM (`node`, `way`, or `relation`)|
| `name`              | string    | Name of the store                                |
| `brand`             | string    | Store brand/type                                 |
| `street`            | string    | Street name                                      |
| `housenumber`       | string    | House number                                     |
| `postcode`          | string    | Postal code                                      |
| `city`              | string    | City (Berlin)                                    |
| `district_id`       | string    | FK â†’ `districts(id)`                             |
| `neighborhood_id`   | string    | FK â†’ `neighborhoods(id)`                         |
| `latitude`          | FLOAT     | Latitude                                         |
| `longitude`         | FLOAT     | Longitude                                        |
| `full_address`      | string    | Combined full address                            |
| `opening_hours`     | string    | Opening hours string                             |
| `phone`             | string    | Contact number                                   |
| `email`             | string    | Contact email                                    |
| `website`           | string    | Website                                          |
| `source`            | string    | Always `osm`                                     |

---

## ğŸ’¾ 4. Insert into Test Database (`test_berlin_data`)

- âœ… Used `SQLAlchemy` and `psycopg2` to connect to NeonDB
- âœ… Verified DB connection and tested inserts with sample records
- âœ… Ensured:
  - Primary key constraints (`store_id`)
  - Foreign key references (`district_id`, `neighborhood_id`)
  - Address + location fields are present
  - Row count matched cleaned dataset

---

## ğŸš§ Next Steps

- [ ] Run data validation queries in SQL (duplicates, nulls, invalid FKs)
- [ ] Insert into production schema after testing
- [ ] Create API endpoints to expose this data (optional)
- [ ] Add documentation for querying spatial data with PostGIS

---

## ğŸ“ Folder structure:

â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ supermarkets_raw.csv
â”‚   â”œâ”€â”€ supermarkets_raw.geojson
â”‚   â””â”€â”€ lor_ortsteile.geojson
    â””â”€â”€ final_supermarkets_with_district.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ supermarkets-data-transforming.ipynb
â”‚   
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

# Step 3:  ğŸ¬ Berlin Supermarkets Data â€“ PostgreSQL Database Ingestion

This project involves loading cleaned supermarket data into a  PostgreSQL database.  
The data is transformed and enriched (e.g. with district and neighborhood info), then inserted into a relational schema using SQLAlchemy and Pandas.

---

## âœ… Step-by-Step Summary

### 1. **Connect to database**

Create a SQLAlchemy engine using  PostgreSQL DBeaver database connection string:

```python
from sqlalchemy import create_engine

DATABASE_URL = "postgresql+psycopg2://<user>:<password>@<host>:5432/<dbname>?sslmode=require"
engine = create_engine(DATABASE_URL)
```

---

### 2. **Create Table with Schema**
Create the `supermarkets` table inside schema `berlin_source_data`.  
It includes columns for store details and foreign key constraints.

```sql
CREATE TABLE IF NOT EXISTS berlin_source_data.supermarkets (
    store_id TEXT PRIMARY KEY,
    store_name TEXT,
    street TEXT,
    housenumber TEXT,
    postcode TEXT,
    city TEXT,
    opening_hours TEXT,
    brand TEXT,
    type TEXT,
    payment_credit_card TEXT,
    payment_debit_cards TEXT,
    payment_cash TEXT,
    payment_contactless TEXT,
    wheelchair TEXT,
    internet_access TEXT,
    phone TEXT,
    email TEXT,
    website TEXT,
    latitude DOUBLE PRECISION,
    longitude DOUBLE PRECISION,
    source TEXT,
    district TEXT,
    neighborhood TEXT,
    neighborhood_id TEXT,
    district_id TEXT,
    CONSTRAINT district_id_fk FOREIGN KEY (district_id)
        REFERENCES berlin_source_data.districts(district_id)
        ON DELETE RESTRICT
        ON UPDATE CASCADE
);
```

---

### 3. **Load Data into Pandas**
```python
import pandas as pd

df_final = pd.read_csv("../sources/final_supermarkets_with_district.csv")
```

---

### 4. **Insert Data into DB**
```python
df_final.to_sql(
    name="supermarkets",
    con=engine,
    schema="berlin_source_data",
    if_exists="append",  # Use "replace" to drop and recreate
    index=False
)
```

---

## ğŸ§± Related Tables & Relationships

- **`districts`** (in `berlin_source_data`): referenced via `district_id`
- **`neighborhoods`**: optionally used via `neighborhood_id` (not yet constrained)

---

## ğŸ›  Requirements

- Python 3.10+
- `sqlalchemy`
- `pandas`
- `psycopg2-binary`

Install:
```bash
pip install sqlalchemy pandas psycopg2-binary
```

---

## ğŸ“ Files

- `final_supermarkets_with_district.csv` â€“ transformed and cleaned data
- Python notebook/script to create table and insert data

---

## ğŸ“Œ Notes

- Ensure that the `berlin_source_data` schema **already exists** in your database.
- Foreign key constraints require referenced tables (`districts`, etc.) to exist and be correctly typed.
- Use `store_id` as unique identifier â€” avoid duplicates to prevent `UniqueViolation` errors.
