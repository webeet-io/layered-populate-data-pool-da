# ğŸ›’ Berlin Superstore Dataset â€“ Data Profiling & Preparation**

This project involves extracting, exploring, and transforming supermarket data in Berlin using OpenStreetMap (OSM) via the osmnx library. The processed dataset can be used for location-based services, business mapping, or urban retail studies.

â¸»

## ğŸ“Œ 1. Data Source Discovery

	â€¢	Source: OpenStreetMap (OSM)
	â€¢	Accessed via: osmnx Python package
	â€¢	Query Tags: {"shop": "supermarket"}
	â€¢	Place Queried: "Berlin, Germany"


Topic: Supermarkets in Berlin

**Main source:**

- Name: OpenStreetMap (OSM) via OSMnx library
- Source and origin: Public crowdsourced geospatial database
- Update frequency: Continuous (dynamic)
- Data type: Dynamic (API query using shop=supermarket)

**Reason for selection:**

- Covers all supermarkets in Berlin
- Includes coordinates, names, addresses, and other useful attributes
- Open, free, and easy to query programmatically

**Optional additional sources:**

- Name: Berlin Open Data Portal (daten.berlin.de)
- Source and origin: Official Berlin city government
- Update frequency: Varies per dataset
- Data type: Static or semi-static (download as CSV/GeoJSON)
- Possible usage: Enrich with official administrative boundaries or extra metadata

**Enrichment potential:**

Neighborhood/district info from Berlin shapefiles (GeoJSON)
Linking to local amenities for spatial context

â¸»

## ğŸ”„ 2. Planned Transformation & Profiling Steps

**âœ… Step-by-Step Process**

	1.	ğŸ—ºï¸ Data Extraction

	â€¢	Queried Berlin OSM boundaries using osmnx.features_from_place(...)
	â€¢	Filtered features using shop=supermarket tag

	2.	ğŸ§¹ Initial Cleaning & Column Selection

	â€¢	Extracted relevant columns:
name, addr:street, addr:housenumber, addr:postcode, addr:city, opening_hours, brand, geometry, and various payment:* tags
	â€¢	Extracted latitude and longitude from geometry
	â€¢	Renamed columns for clarity (e.g., addr:street â†’ street, brand â†’ store_type, etc.)

	3.	ğŸ“‹ Dataset Profiling

	â€¢	.shape â€“ Number of rows and columns
	â€¢	.info() â€“ Data types and null counts
	â€¢	Missing value analysis â€“ Count + percentage of missing values per column
	â€¢	Distinct values per column â€“ For understanding cardinality
	â€¢	Most common values â€“ Using value_counts() for categorical profiling

	4.	ğŸ“ Spatial Sanity Checks

	â€¢	Verified geometry types (Point vs MultiPoint)
	â€¢	Confirmed latitude and longitude ranges fall within Berlinâ€™s bounding box

	5.	ğŸ’¾ Data Export

	â€¢	Raw dataset saved in sources folder as:
	â€¢	CSV: supermarkets_raw.csv without geometry
	â€¢	GeoJSON: supermarkets_raw.geojson with geometry (for mapping use)

â¸»

## ğŸ§° Tools Used**

	â€¢	Python
	â€¢	osmnx, geopandas, pandas
	â€¢	Jupyter Notebook (for analysis and exploration)

# ğŸª Berlin Superstores â€“ Data Transformation & DB Insert

This project documents the transformation and integration process for a dataset of supermarkets (and similar stores) in Berlin. The data was sourced from **OpenStreetMap (OSM)** using the **Overpass API** and further enriched with geographic and administrative context before being inserted into a **Neon test database**.

---

## ğŸ—‚ï¸ 1. Data Source

- âœ… **Source**: OpenStreetMap (via `osmnx` and Overpass API)
- âœ… **Tags used**: `shop=supermarket`, `convenience`, `grocery`, `department_store`, `general`
- âœ… **Format**: JSON â†’ GeoDataFrame â†’ Cleaned DataFrame

---

## ğŸ”„ 2. Data Transformation Steps

All transformation steps were handled in Python using `pandas` and `geopandas`.

### âœ… Cleaning & Normalization
- Cleaned column names (lowercase, snake_case, removed special characters)
- Standardized string fields (`name`, `brand`, `address`, etc.)
- Converted `NaN` to `None` where needed for DB compatibility

### âœ… Enrichment
- Extracted `latitude` and `longitude` from geometry
- Reverse geocoded missing address fields (`street`, `housenumber`, `postcode`, `city`)
- Joined **district** and **neighborhood** info using Berlin LOR GeoJSON boundaries
- Added new fields: `source`, `store_id`, `full_address`

### âœ… Data Validation
- Checked data types and missing values
- Ensured geometry points are valid and matched with admin boundaries
- Normalized contact fields (`phone`, `email`, `website`)
- Dropped irrelevant or overly sparse columns

---

## ğŸ—ƒï¸ 3. Planned Database Schema (`superstore_in_berlin`)

| Column              | Type      | Description                                      |
|---------------------|-----------|-----------------------------------------------   |
| `store_id`          | string    | Unique ID from OSM (`node`, `way`, or `relation`)|
| `name`              | string    | Name of the store                                |
| `brand`             | string    | Store brand/type                                 |
| `street`            | string    | Street name                                      |
| `housenumber`       | string    | House number                                     |
| `postcode`          | string    | Postal code                                      |
| `city`              | string    | City (Berlin)                                    |
| `district_id`       | string    | FK â†’ `districts(id)`                             |
| `neighborhood_id`   | string    | FK â†’ `neighborhoods(id)`                         |
| `latitude`          | FLOAT     | Latitude                                         |
| `longitude`         | FLOAT     | Longitude                                        |
| `full_address`      | string    | Combined full address                            |
| `opening_hours`     | string    | Opening hours string                             |
| `phone`             | string    | Contact number                                   |
| `email`             | string    | Contact email                                    |
| `website`           | string    | Website                                          |
| `source`            | string    | Always `osm`                                     |

---

## ğŸ’¾ 4. Insert into Test Database (`test_berlin_data`)

- âœ… Used `SQLAlchemy` and `psycopg2` to connect to NeonDB
- âœ… Verified DB connection and tested inserts with sample records
- âœ… Ensured:
  - Primary key constraints (`store_id`)
  - Foreign key references (`district_id`, `neighborhood_id`)
  - Address + location fields are present
  - Row count matched cleaned dataset

---

## ğŸš§ Next Steps

- [ ] Run data validation queries in SQL (duplicates, nulls, invalid FKs)
- [ ] Insert into production schema after testing
- [ ] Create API endpoints to expose this data (optional)
- [ ] Add documentation for querying spatial data with PostGIS

---

## ğŸ“ Folder structure:

â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ supermarkets_raw.csv
â”‚   â”œâ”€â”€ supermarkets_raw.geojson
â”‚   â””â”€â”€ lor_ortsteile.geojson
    â””â”€â”€ final_supermarkets_with_district.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ supermarkets-data-transforming.ipynb
â”‚   
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt