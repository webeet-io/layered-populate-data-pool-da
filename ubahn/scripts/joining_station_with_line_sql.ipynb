{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merged_ubahn_line.csv updated and saved with manual coordinates.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# First, I load both the cleaned connections file and the stations file which already includes postcodes\n",
    "connections_df = pd.read_csv(\"08-connections-no-dupes.csv\")\n",
    "stations_df = pd.read_csv(\"simoun-asmar-berlin-stations-with-postcode.csv\")\n",
    "\n",
    "# Then I define a helper function to remove known prefixes and standardize station names\n",
    "def clean_name(name):\n",
    "    return (\n",
    "        name.replace(\"U-Bahnhof \", \"\")\n",
    "            .replace(\"Bahnhof Berlin \", \"\")\n",
    "            .replace(\"Bahnhöfe Berlin \", \"\")\n",
    "            .replace(\"S-Bahnhof \", \"\")\n",
    "            .replace(\"Bahnhof \", \"\")\n",
    "            .replace(\"Berlin-\", \"\")\n",
    "            .strip()\n",
    "    )\n",
    "\n",
    "# I apply this cleaning function to both datasets so the station names can be matched later\n",
    "connections_df[\"point1_clean\"] = connections_df[\"point1\"].apply(clean_name)\n",
    "stations_df[\"station_clean\"] = stations_df[\"name\"].apply(clean_name)\n",
    "\n",
    "# At this stage, I remove any entries that still start with Q* IDs (likely leftover Wikidata entries)\n",
    "connections_df = connections_df[~connections_df[\"point1_clean\"].str.startswith(\"Q\")]\n",
    "stations_df = stations_df[~stations_df[\"station_clean\"].str.startswith(\"Q\")]\n",
    "\n",
    "# I make sure all postcodes are stored as nullable integers for consistency\n",
    "stations_df[\"postcode\"] = pd.to_numeric(stations_df[\"postcode\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Then I drop any duplicate station names that might have slipped through\n",
    "stations_df = stations_df.drop_duplicates(subset=[\"station_clean\"])\n",
    "\n",
    "# I now initialize a local SQLite database to merge and query the data efficiently\n",
    "conn = sqlite3.connect(\"berlin_transport.db\")\n",
    "\n",
    "# Next, I push both datasets (connections and stations) into two separate tables in SQLite\n",
    "connections_df[[\"point1_clean\", \"line\"]].drop_duplicates()\\\n",
    "    .to_sql(\"connections\", conn, if_exists=\"replace\", index=False)\n",
    "stations_df[[\"station_clean\", \"latitude\", \"longitude\", \"postcode\"]]\\\n",
    "    .to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# I then write an SQL query to join stations and lines, enriching the line data with coordinates and postcodes\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.point1_clean AS station,\n",
    "    c.line,\n",
    "    s.latitude,\n",
    "    s.longitude,\n",
    "    s.postcode\n",
    "FROM connections c\n",
    "LEFT JOIN stations s\n",
    "    ON TRIM(c.point1_clean) = TRIM(s.station_clean)\n",
    "ORDER BY station\n",
    "\"\"\"\n",
    "\n",
    "# I run the query and load the result into a new DataFrame\n",
    "result_df = pd.read_sql_query(query, conn)\n",
    "# Postcodes were float values ending with '.0', converting to string and removing suffix\n",
    "result_df[\"postcode\"] = result_df[\"postcode\"].astype(str).str.replace(\".0\", \"\", regex=False)\n",
    "\n",
    "# After the join, I manually patch missing coordinates for specific stations that couldn’t be matched\n",
    "manual_coords = {\n",
    "    \"Beusselstraße\": (52.534444, 13.329444),\n",
    "    \"Charlottenburg\": (52.50505, 13.30452),\n",
    "    \"Fehrbelliner Platz (U3)\": (52.4897, 13.3153),\n",
    "    \"Fehrbelliner Platz (U7)\": (52.4897, 13.3153),\n",
    "    \"Friedrichstraße\": (52.52000, 13.38700),\n",
    "    \"Hackescher Markt\": (52.52333, 13.40278),\n",
    "    \"Hermannstraße\": (52.46722, 13.43194),\n",
    "    \"Hohenzollerndamm\": (52.48722, 13.31250),\n",
    "    \"Köllnische Heide\": (52.46583, 13.46056),\n",
    "    \"Ostkreuz\": (52.50278, 13.46917),\n",
    "    \"Südkreuz\": (52.47500, 13.36500),\n",
    "    \"Zoologischer Garten\": (52.50750, 13.33417),\n",
    "}\n",
    "\n",
    "for station, (lat, lon) in manual_coords.items():\n",
    "    mask = result_df[\"station\"] == station\n",
    "    result_df.loc[mask, \"latitude\"] = lat\n",
    "    result_df.loc[mask, \"longitude\"] = lon\n",
    "\n",
    "# Finally, I export the enriched station-line dataset to a new CSV that will be used in the next transformation step\n",
    "result_df.to_csv(\"merged_ubahn_line.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
